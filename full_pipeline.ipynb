{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# Google Colab Setup — Install everything needed for Task 2 & 3\n# ============================================================\nimport os, sys\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running on Google Colab — setting up environment...\")\n\n    # 1. Install Python packages\n    !pip install -q pythainlp matplotlib seaborn pandas numpy networkx ollama\n\n    # 2. Install Thai fonts for matplotlib\n    !apt-get install -y fonts-tlwg-sarabun fonts-noto-thai > /dev/null 2>&1\n    import matplotlib, shutil, pathlib\n    cache_dir = pathlib.Path(matplotlib.get_cachedir())\n    if cache_dir.exists():\n        shutil.rmtree(cache_dir)\n    print(\"Thai fonts installed.\")\n\n    # 3. Install & start Ollama\n    !sudo apt-get install -y zstd > /dev/null 2>&1\n    !curl -fsSL https://ollama.com/install.sh | sh\n    import subprocess, time\n    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    time.sleep(5)\n    !ollama pull qwen3:8b\n    print(\"Ollama + qwen3:8b ready.\")\n\n    # 4. Clone project data from GitHub\n    if not os.path.exists('RAG'):\n        !git clone https://github.com/warisara29/RAG.git\n    os.chdir('RAG')\n\n    print(f\"\\nSetup complete! Working directory: {os.getcwd()}\")\nelse:\n    print(\"Running locally — skipping Colab setup.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Collection & Preprocessing\n",
    "## Thai Criminal Law Q&A Chatbot — PageIndex vs PageIndex + Light Knowledge Graph\n",
    "\n",
    "**Course:** CS652 Applied Machine Learning  \n",
    "**Student:** วริศรา พิลาสุข 6809036087  \n",
    "**Date:** February 2026\n",
    "\n",
    "---\n",
    "\n",
    "This notebook covers the data collection and preprocessing phase:\n",
    "1. Download the Thai Criminal Code dataset\n",
    "2. Explore and clean the data\n",
    "3. Parse the hierarchical structure (ภาค > ลักษณะ > หมวด > มาตรา)\n",
    "4. Extract legal entities for the Knowledge Graph\n",
    "5. Produce preprocessed files for the PageIndex and KG pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Thai NLP\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import normalize as thai_normalize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import font_manager\n",
    "\n",
    "# Graph\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "print('All imports OK')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure Thai font for matplotlib\n",
    "thai_font_candidates = ['Sarabun', 'TH Sarabun New', 'Tahoma', 'Noto Sans Thai',\n",
    "                        'Angsana New', 'Cordia New', 'FreeSans']\n",
    "available_fonts = {f.name for f in font_manager.fontManager.ttflist}\n",
    "\n",
    "thai_font = None\n",
    "for name in thai_font_candidates:\n",
    "    if name in available_fonts:\n",
    "        thai_font = name\n",
    "        break\n",
    "\n",
    "if thai_font:\n",
    "    plt.rcParams['font.family'] = thai_font\n",
    "    print(f'Using Thai font: {thai_font}')\n",
    "else:\n",
    "    # Try any font containing 'thai' in the name\n",
    "    for f in font_manager.fontManager.ttflist:\n",
    "        if 'thai' in f.name.lower() or 'sarabun' in f.name.lower():\n",
    "            plt.rcParams['font.family'] = f.name\n",
    "            thai_font = f.name\n",
    "            print(f'Using Thai font: {f.name}')\n",
    "            break\n",
    "    if not thai_font:\n",
    "        print('WARNING: No Thai font found. Thai text in plots may not render correctly.')\n",
    "        print('Consider installing: pip install thai-font  or download Sarabun font')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style('whitegrid')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition\n",
    "\n",
    "The Thai Criminal Code (ประมวลกฎหมายอาญา) dataset is sourced from the [PyThaiNLP/thai-law](https://github.com/PyThaiNLP/thai-law) repository.  \n",
    "Original data is from the Office of the Council of State (สำนักงานคณะกรรมการกฤษฎีกา).  \n",
    "The data is public domain under Thai Copyright Act Article 7."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_URL = 'https://github.com/PyThaiNLP/thai-law/releases/download/criminal-csv-v0.1/criminal-datasets.csv'\n",
    "DATA_DIR = 'data'\n",
    "RAW_FILE = os.path.join(DATA_DIR, 'criminal-datasets.csv')\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(RAW_FILE):\n",
    "    import urllib.request\n",
    "    print(f'Downloading from {DATA_URL}...')\n",
    "    urllib.request.urlretrieve(DATA_URL, RAW_FILE)\n",
    "    print(f'Saved to {RAW_FILE}')\n",
    "else:\n",
    "    print(f'File already exists: {RAW_FILE}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_raw = pd.read_csv(RAW_FILE)\n",
    "print(f'Shape: {df_raw.shape}')\n",
    "print(f'Columns: {df_raw.columns.tolist()}')\n",
    "df_raw.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('=== Data Types ===')\n",
    "print(df_raw.dtypes)\n",
    "print(f'\\n=== Missing Values ===')\n",
    "print(df_raw.isnull().sum())\n",
    "print(f'\\nTotal rows: {len(df_raw)}')\n",
    "print(f'Unique articles: {df_raw[\"article\"].nunique()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Separate intro sections from numbered articles\n",
    "intro_mask = df_raw['article'].astype(str).str.startswith('intro')\n",
    "print(f'Intro (preamble) sections: {intro_mask.sum()}')\n",
    "print(f'Main code entries: {(~intro_mask).sum()}')\n",
    "\n",
    "print('\\n=== Sample intro sections ===')\n",
    "display(df_raw[intro_mask].head())\n",
    "\n",
    "print('\\n=== Sample main articles ===')\n",
    "display(df_raw[~intro_mask].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Text length analysis\n",
    "df_raw['text_length'] = df_raw['text'].astype(str).str.len()\n",
    "print('=== Text Length Statistics ===')\n",
    "print(df_raw['text_length'].describe())\n",
    "\n",
    "print(f'\\n=== Notes column ===')\n",
    "print(f'Non-null notes: {df_raw[\"notes\"].notna().sum()}')\n",
    "print(f'Null notes: {df_raw[\"notes\"].isna().sum()}')\n",
    "if df_raw['notes'].notna().any():\n",
    "    print('\\nSample notes:')\n",
    "    display(df_raw[df_raw['notes'].notna()][['article', 'notes']].head(10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = df_raw.copy()\n",
    "\n",
    "# 1. Handle missing values\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['notes'] = df['notes'].fillna('')\n",
    "print(f'Missing values after fill: {df.isnull().sum().sum()}')\n",
    "\n",
    "# 2. Text normalization using PyThaiNLP\n",
    "df['text_clean'] = df['text'].apply(lambda x: thai_normalize(str(x)))\n",
    "\n",
    "# 3. Remove extra whitespace (preserve Thai text structure)\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# 4. Check for duplicates\n",
    "dup_articles = df['article'].duplicated().sum()\n",
    "dup_texts = df['text_clean'].duplicated().sum()\n",
    "print(f'Duplicate article IDs: {dup_articles}')\n",
    "print(f'Duplicate texts: {dup_texts}')\n",
    "\n",
    "# 5. Remove empty text rows\n",
    "empty_mask = df['text_clean'].str.strip() == ''\n",
    "print(f'Empty text rows: {empty_mask.sum()}')\n",
    "if empty_mask.sum() > 0:\n",
    "    df = df[~empty_mask].reset_index(drop=True)\n",
    "\n",
    "print(f'\\nShape after cleaning: {df.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Structural Hierarchy Parsing\n",
    "\n",
    "The Thai Criminal Code has a hierarchical structure:\n",
    "- **ภาค** (Book) — ภาค 1 บทบัญญัติทั่วไป, ภาค 2 ความผิด, ภาค 3 ลหุโทษ\n",
    "- **ลักษณะ** (Title) — groups of related offenses\n",
    "- **หมวด** (Division) — subdivisions within a title\n",
    "- **มาตรา** (Section) — individual legal provisions\n",
    "\n",
    "The CSV contains only flat rows of มาตรา without structural headers.  \n",
    "We use the **official Thai Criminal Code structure** to map each section to its Book/Title/Division."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Thai numeral conversion\n",
    "THAI_DIGITS = '๐๑๒๓๔๕๖๗๘๙'\n",
    "ARABIC_DIGITS = '0123456789'\n",
    "\n",
    "def thai_to_arabic(text):\n",
    "    \"\"\"Convert Thai numerals to Arabic numerals.\"\"\"\n",
    "    for thai, arabic in zip(THAI_DIGITS, ARABIC_DIGITS):\n",
    "        text = text.replace(thai, arabic)\n",
    "    return text\n",
    "\n",
    "def extract_number(text):\n",
    "    \"\"\"Extract the first number (Thai or Arabic) from text.\"\"\"\n",
    "    converted = thai_to_arabic(str(text))\n",
    "    match = re.search(r'\\d+', converted)\n",
    "    return int(match.group()) if match else None\n",
    "\n",
    "# Test\n",
    "print(thai_to_arabic('มาตรา ๓๙๘'))  # -> มาตรา 398\n",
    "print(extract_number('๓๙๘'))  # -> 398"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Official Thai Criminal Code hierarchy mapping\n",
    "# Each entry: (start_section, end_section, book_num, book_name, title_num, title_name, division_num, division_name)\n",
    "# Based on the official structure from the Office of the Council of State\n",
    "\n",
    "HIERARCHY_MAP = [\n",
    "    # ===== ภาค 1: บทบัญญัติทั่วไป (General Provisions) =====\n",
    "    # ลักษณะ 1: บทบัญญัติที่ใช้แก่ความผิดทั่วไป\n",
    "    (1,   1,   1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 1, 'บทนิยาม'),\n",
    "    (2,   17,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 2, 'การใช้กฎหมายอาญา'),\n",
    "    (18,  38,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 3, 'โทษและวิธีการเพื่อความปลอดภัย'),\n",
    "    (39,  50,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 4, 'วิธีการเพื่อความปลอดภัย'),\n",
    "    (51,  58,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 5, 'การเพิ่มโทษ ลดโทษ และการรอการลงโทษ'),\n",
    "    (59,  79,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 6, 'ความรับผิดในทางอาญา'),\n",
    "    (80,  82,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 7, 'การพยายามกระทำความผิด'),\n",
    "    (83,  89,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 8, 'ตัวการและผู้สนับสนุน'),\n",
    "    (90,  91,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 9, 'การกระทำความผิดหลายบทหรือหลายกระทง'),\n",
    "    (92,  94,  1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 10, 'การกระทำความผิดอีก'),\n",
    "    (95,  101, 1, 'บทบัญญัติทั่วไป', 1, 'บทบัญญัติที่ใช้แก่ความผิดทั่วไป', 11, 'อายุความ'),\n",
    "    # ลักษณะ 2: บทบัญญัติที่ใช้แก่ความผิดลหุโทษ\n",
    "    (102, 106, 1, 'บทบัญญัติทั่วไป', 2, 'บทบัญญัติที่ใช้แก่ความผิดลหุโทษ', None, None),\n",
    "\n",
    "    # ===== ภาค 2: ความผิด (Offenses) =====\n",
    "    # ลักษณะ 1: ความผิดเกี่ยวกับความมั่นคงแห่งราชอาณาจักร\n",
    "    (107, 112, 2, 'ความผิด', 1, 'ความผิดเกี่ยวกับความมั่นคงแห่งราชอาณาจักร', 1, 'ความผิดต่อองค์พระมหากษัตริย์ พระราชินี รัชทายาท และผู้สำเร็จราชการแทนพระองค์'),\n",
    "    (113, 118, 2, 'ความผิด', 1, 'ความผิดเกี่ยวกับความมั่นคงแห่งราชอาณาจักร', 2, 'ความผิดต่อความมั่นคงของรัฐภายในราชอาณาจักร'),\n",
    "    (119, 129, 2, 'ความผิด', 1, 'ความผิดเกี่ยวกับความมั่นคงแห่งราชอาณาจักร', 3, 'ความผิดต่อความมั่นคงของรัฐภายนอกราชอาณาจักร'),\n",
    "    (130, 135, 2, 'ความผิด', 1, 'ความผิดเกี่ยวกับความมั่นคงแห่งราชอาณาจักร', 4, 'ความผิดต่อสัมพันธไมตรีกับต่างประเทศ'),\n",
    "    # ลักษณะ 1/1: ความผิดเกี่ยวกับการก่อการร้าย (sections 135/1-135/4 use base=135)\n",
    "    # These are handled by the slash logic below\n",
    "\n",
    "    # ลักษณะ 2: ความผิดเกี่ยวกับการปกครอง\n",
    "    (136, 146, 2, 'ความผิด', 2, 'ความผิดเกี่ยวกับการปกครอง', 1, 'ความผิดต่อเจ้าพนักงาน'),\n",
    "    (147, 166, 2, 'ความผิด', 2, 'ความผิดเกี่ยวกับการปกครอง', 2, 'ความผิดต่อตำแหน่งหน้าที่ราชการ'),\n",
    "\n",
    "    # ลักษณะ 3: ความผิดเกี่ยวกับการยุติธรรม\n",
    "    (167, 199, 2, 'ความผิด', 3, 'ความผิดเกี่ยวกับการยุติธรรม', 1, 'ความผิดต่อเจ้าพนักงานในการยุติธรรม'),\n",
    "    (200, 205, 2, 'ความผิด', 3, 'ความผิดเกี่ยวกับการยุติธรรม', 2, 'ความผิดต่อตำแหน่งหน้าที่ในการยุติธรรม'),\n",
    "\n",
    "    # ลักษณะ 4: ความผิดเกี่ยวกับศาสนา\n",
    "    (206, 208, 2, 'ความผิด', 4, 'ความผิดเกี่ยวกับศาสนา', None, None),\n",
    "\n",
    "    # ลักษณะ 5: ความผิดเกี่ยวกับความสงบสุขของประชาชน\n",
    "    (209, 216, 2, 'ความผิด', 5, 'ความผิดเกี่ยวกับความสงบสุขของประชาชน', None, None),\n",
    "\n",
    "    # ลักษณะ 6: ความผิดเกี่ยวกับการก่อให้เกิดภยันตรายต่อประชาชน\n",
    "    (217, 239, 2, 'ความผิด', 6, 'ความผิดเกี่ยวกับการก่อให้เกิดภยันตรายต่อประชาชน', None, None),\n",
    "\n",
    "    # ลักษณะ 7: ความผิดเกี่ยวกับการปลอมและการแปลง\n",
    "    (240, 249, 2, 'ความผิด', 7, 'ความผิดเกี่ยวกับการปลอมและการแปลง', 1, 'ความผิดเกี่ยวกับเงินตรา'),\n",
    "    (250, 263, 2, 'ความผิด', 7, 'ความผิดเกี่ยวกับการปลอมและการแปลง', 2, 'ความผิดเกี่ยวกับดวงตรา แสตมป์ และตั๋ว'),\n",
    "    (264, 269, 2, 'ความผิด', 7, 'ความผิดเกี่ยวกับการปลอมและการแปลง', 3, 'ความผิดเกี่ยวกับเอกสาร'),\n",
    "    # 269/1-269/15: ความผิดเกี่ยวกับบัตรอิเล็กทรอนิกส์ (handled by slash logic)\n",
    "\n",
    "    # ลักษณะ 8: ความผิดเกี่ยวกับการค้า\n",
    "    (270, 275, 2, 'ความผิด', 8, 'ความผิดเกี่ยวกับการค้า', None, None),\n",
    "\n",
    "    # ลักษณะ 9: ความผิดเกี่ยวกับเพศ\n",
    "    (276, 287, 2, 'ความผิด', 9, 'ความผิดเกี่ยวกับเพศ', None, None),\n",
    "\n",
    "    # ลักษณะ 10: ความผิดเกี่ยวกับชีวิตและร่างกาย\n",
    "    (288, 294, 2, 'ความผิด', 10, 'ความผิดเกี่ยวกับชีวิตและร่างกาย', 1, 'ความผิดต่อชีวิต'),\n",
    "    (295, 300, 2, 'ความผิด', 10, 'ความผิดเกี่ยวกับชีวิตและร่างกาย', 2, 'ความผิดต่อร่างกาย'),\n",
    "    (301, 305, 2, 'ความผิด', 10, 'ความผิดเกี่ยวกับชีวิตและร่างกาย', 3, 'ความผิดฐานทำให้แท้งลูก'),\n",
    "    (306, 308, 2, 'ความผิด', 10, 'ความผิดเกี่ยวกับชีวิตและร่างกาย', 4, 'ความผิดฐานทอดทิ้งเด็ก คนป่วยเจ็บ หรือคนชรา'),\n",
    "\n",
    "    # ลักษณะ 11: ความผิดเกี่ยวกับเสรีภาพและชื่อเสียง\n",
    "    (309, 321, 2, 'ความผิด', 11, 'ความผิดเกี่ยวกับเสรีภาพและชื่อเสียง', 1, 'ความผิดต่อเสรีภาพ'),\n",
    "    (322, 325, 2, 'ความผิด', 11, 'ความผิดเกี่ยวกับเสรีภาพและชื่อเสียง', 2, 'ความผิดฐานเปิดเผยความลับ'),\n",
    "    (326, 333, 2, 'ความผิด', 11, 'ความผิดเกี่ยวกับเสรีภาพและชื่อเสียง', 3, 'ความผิดฐานหมิ่นประมาท'),\n",
    "\n",
    "    # ลักษณะ 12: ความผิดเกี่ยวกับทรัพย์\n",
    "    (334, 336, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 1, 'ความผิดฐานลักทรัพย์และวิ่งราวทรัพย์'),\n",
    "    (337, 340, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 2, 'ความผิดฐานกรรโชก รีดเอาทรัพย์ ชิงทรัพย์ และปล้นทรัพย์'),\n",
    "    (341, 348, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 3, 'ความผิดฐานฉ้อโกง'),\n",
    "    (349, 351, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 4, 'ความผิดฐานโกงเจ้าหนี้'),\n",
    "    (352, 356, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 5, 'ความผิดฐานยักยอก'),\n",
    "    (357, 357, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 6, 'ความผิดฐานรับของโจร'),\n",
    "    (358, 361, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 7, 'ความผิดฐานทำให้เสียทรัพย์'),\n",
    "    (362, 366, 2, 'ความผิด', 12, 'ความผิดเกี่ยวกับทรัพย์', 8, 'ความผิดฐานบุกรุก'),\n",
    "\n",
    "    # ===== ภาค 3: ลหุโทษ (Petty Offenses) =====\n",
    "    (367, 398, 3, 'ลหุโทษ', None, None, None, None),\n",
    "]\n",
    "\n",
    "# Special mapping for slash articles (135/x -> ลักษณะ 1/1 ความผิดเกี่ยวกับการก่อการร้าย)\n",
    "SLASH_OVERRIDES = {\n",
    "    135: (2, 'ความผิด', '1/1', 'ความผิดเกี่ยวกับการก่อการร้าย', None, None),\n",
    "    269: (2, 'ความผิด', 7, 'ความผิดเกี่ยวกับการปลอมและการแปลง', 4, 'ความผิดเกี่ยวกับบัตรอิเล็กทรอนิกส์'),\n",
    "}\n",
    "\n",
    "def get_base_section(section_num_str):\n",
    "    \"\"\"Extract the base section number (before /) as int.\"\"\"\n",
    "    return int(section_num_str.split('/')[0])\n",
    "\n",
    "def map_section_to_hierarchy(section_num_str):\n",
    "    \"\"\"Map a section number to its Book/Title/Division using the official structure.\"\"\"\n",
    "    is_slash = '/' in section_num_str\n",
    "    base = get_base_section(section_num_str)\n",
    "    \n",
    "    # Check slash overrides first\n",
    "    if is_slash and base in SLASH_OVERRIDES:\n",
    "        ov = SLASH_OVERRIDES[base]\n",
    "        return {\n",
    "            'book_num': ov[0], 'book_name': ov[1],\n",
    "            'title_num': ov[2], 'title_name': ov[3],\n",
    "            'division_num': ov[4], 'division_name': ov[5],\n",
    "        }\n",
    "    \n",
    "    # Search in the hierarchy map\n",
    "    for (start, end, b_num, b_name, t_num, t_name, d_num, d_name) in HIERARCHY_MAP:\n",
    "        if start <= base <= end:\n",
    "            return {\n",
    "                'book_num': b_num, 'book_name': b_name,\n",
    "                'title_num': t_num, 'title_name': t_name,\n",
    "                'division_num': d_num, 'division_name': d_name,\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'book_num': None, 'book_name': None,\n",
    "        'title_num': None, 'title_name': None,\n",
    "        'division_num': None, 'division_name': None,\n",
    "    }\n",
    "\n",
    "# Test\n",
    "print(map_section_to_hierarchy('1'))\n",
    "print(map_section_to_hierarchy('334'))\n",
    "print(map_section_to_hierarchy('135/1'))\n",
    "print(map_section_to_hierarchy('269/5'))\n",
    "print(map_section_to_hierarchy('398'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def parse_all_sections(df):\n",
    "    \"\"\"\n",
    "    Parse all rows: extract section numbers and map to hierarchy.\n",
    "    Handles intro sections (preamble act) separately.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row['text_clean']).strip()\n",
    "        article_id = str(row['article']).strip()\n",
    "        \n",
    "        # Determine if intro or main section\n",
    "        is_intro = article_id.startswith('intro')\n",
    "        \n",
    "        # Extract section number from text\n",
    "        section_num = None\n",
    "        section_text = text\n",
    "        sec_match = re.search(r'มาตรา\\s*([๐-๙\\d]+(?:/[๐-๙\\d]+)?)', text)\n",
    "        if sec_match:\n",
    "            section_num = thai_to_arabic(sec_match.group(1))\n",
    "            section_text = text[sec_match.end():].strip()\n",
    "        \n",
    "        # Map to hierarchy\n",
    "        if section_num and not is_intro:\n",
    "            hierarchy = map_section_to_hierarchy(section_num)\n",
    "        else:\n",
    "            hierarchy = {\n",
    "                'book_num': 0 if is_intro else None,\n",
    "                'book_name': 'พ.ร.บ.ให้ใช้ประมวลกฎหมายอาญา' if is_intro else None,\n",
    "                'title_num': None, 'title_name': None,\n",
    "                'division_num': None, 'division_name': None,\n",
    "            }\n",
    "        \n",
    "        records.append({\n",
    "            'article_id': article_id,\n",
    "            'original_text': row['text'],\n",
    "            'text_clean': text,\n",
    "            'notes': row.get('notes', ''),\n",
    "            'is_intro': is_intro,\n",
    "            'section_num': section_num,\n",
    "            'section_text': section_text,\n",
    "            **hierarchy,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "df_parsed = parse_all_sections(df)\n",
    "print(f'Parsed shape: {df_parsed.shape}')\n",
    "print(f'Intro sections: {df_parsed[\"is_intro\"].sum()}')\n",
    "print(f'Main sections with number: {df_parsed[\"section_num\"].notna().sum()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Validate the hierarchy mapping\n",
    "main_sections = df_parsed[~df_parsed['is_intro'] & df_parsed['section_num'].notna()]\n",
    "\n",
    "print('=== Books (ภาค) ==='  )\n",
    "for bnum, bname in sorted(main_sections[['book_num', 'book_name']].drop_duplicates().values, key=lambda x: x[0]):\n",
    "    count = len(main_sections[main_sections['book_num'] == bnum])\n",
    "    print(f'  ภาค {bnum}: {bname} ({count} sections)')\n",
    "\n",
    "print(f'\\n=== Titles (ลักษณะ) ===')\n",
    "title_groups = main_sections.groupby(['book_num', 'book_name', 'title_num', 'title_name']).size().reset_index(name='count')\n",
    "for _, r in title_groups.iterrows():\n",
    "    if pd.notna(r['title_num']):\n",
    "        print(f'  ภาค {int(r[\"book_num\"])} > ลักษณะ {r[\"title_num\"]}: {r[\"title_name\"]} ({r[\"count\"]} sections)')\n",
    "\n",
    "print(f'\\n=== Sections without hierarchy mapping ===')\n",
    "unmapped = main_sections[main_sections['book_num'].isna()]\n",
    "if len(unmapped) > 0:\n",
    "    print(f'  WARNING: {len(unmapped)} sections not mapped!')\n",
    "    print(unmapped[['section_num', 'text_clean']].head())\n",
    "else:\n",
    "    print('  All sections mapped successfully!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Transformation\n",
    "\n",
    "Build two key structures:\n",
    "1. **Nested JSON tree** — for PageIndex tree construction\n",
    "2. **Flat structured DataFrame** — for analysis and KG entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def build_hierarchy_tree(df_sections):\n",
    "    \"\"\"\n",
    "    Build a nested dictionary representing the document hierarchy.\n",
    "    Structure: root > books > titles > divisions > sections\n",
    "    Uses the hierarchy columns already mapped in the DataFrame.\n",
    "    \"\"\"\n",
    "    tree = {\n",
    "        'root': 'ประมวลกฎหมายอาญา',\n",
    "        'type': 'root',\n",
    "        'children': []\n",
    "    }\n",
    "    \n",
    "    # Group by hierarchy levels\n",
    "    book_cache = {}\n",
    "    title_cache = {}\n",
    "    div_cache = {}\n",
    "    \n",
    "    for _, row in df_sections.iterrows():\n",
    "        b_num = row['book_num']\n",
    "        b_name = row['book_name']\n",
    "        t_num = row['title_num']\n",
    "        t_name = row['title_name']\n",
    "        d_num = row['division_num']\n",
    "        d_name = row['division_name']\n",
    "        \n",
    "        # Get or create book node\n",
    "        b_key = b_num\n",
    "        if b_key not in book_cache:\n",
    "            book_node = {\n",
    "                'type': 'book',\n",
    "                'number': b_num,\n",
    "                'name': f'ภาค {b_num} {b_name}',\n",
    "                'children': []\n",
    "            }\n",
    "            book_cache[b_key] = book_node\n",
    "            tree['children'].append(book_node)\n",
    "        book_node = book_cache[b_key]\n",
    "        \n",
    "        # Get or create title node\n",
    "        if pd.notna(t_num):\n",
    "            t_key = (b_num, t_num)\n",
    "            if t_key not in title_cache:\n",
    "                title_node = {\n",
    "                    'type': 'title',\n",
    "                    'number': t_num,\n",
    "                    'name': f'ลักษณะ {t_num} {t_name}',\n",
    "                    'children': []\n",
    "                }\n",
    "                title_cache[t_key] = title_node\n",
    "                book_node['children'].append(title_node)\n",
    "            title_node = title_cache[t_key]\n",
    "        else:\n",
    "            title_node = None\n",
    "        \n",
    "        # Get or create division node\n",
    "        if pd.notna(d_num):\n",
    "            d_key = (b_num, t_num, d_num)\n",
    "            if d_key not in div_cache:\n",
    "                div_node = {\n",
    "                    'type': 'division',\n",
    "                    'number': d_num,\n",
    "                    'name': f'หมวด {d_num} {d_name}',\n",
    "                    'children': []\n",
    "                }\n",
    "                div_cache[d_key] = div_node\n",
    "                if title_node:\n",
    "                    title_node['children'].append(div_node)\n",
    "                else:\n",
    "                    book_node['children'].append(div_node)\n",
    "            div_node = div_cache[d_key]\n",
    "        else:\n",
    "            div_node = None\n",
    "        \n",
    "        # Create section leaf node\n",
    "        section_node = {\n",
    "            'type': 'section',\n",
    "            'number': row['section_num'],\n",
    "            'text': row['section_text'],\n",
    "            'full_text': row['text_clean'],\n",
    "            'notes': str(row['notes']) if row['notes'] else '',\n",
    "            'article_id': row['article_id']\n",
    "        }\n",
    "        \n",
    "        # Attach to deepest available parent\n",
    "        if div_node:\n",
    "            div_node['children'].append(section_node)\n",
    "        elif title_node:\n",
    "            title_node['children'].append(section_node)\n",
    "        else:\n",
    "            book_node['children'].append(section_node)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# Filter to main sections only (exclude intro)\n",
    "df_main = df_parsed[~df_parsed['is_intro'] & df_parsed['section_num'].notna()].copy()\n",
    "hierarchy_tree = build_hierarchy_tree(df_main)\n",
    "\n",
    "# Summary\n",
    "print(f'Tree root: {hierarchy_tree[\"root\"]}')\n",
    "for book in hierarchy_tree['children']:\n",
    "    n_titles = sum(1 for c in book['children'] if c['type'] == 'title')\n",
    "    n_sections_direct = sum(1 for c in book['children'] if c['type'] == 'section')\n",
    "    print(f\"  ภาค {book['number']}: {book['name']}\")\n",
    "    print(f\"    Titles: {n_titles}, Direct sections: {n_sections_direct}\")\n",
    "    for title in book['children']:\n",
    "        if title['type'] == 'title':\n",
    "            n_div = sum(1 for c in title['children'] if c['type'] == 'division')\n",
    "            n_sec = sum(1 for c in title['children'] if c['type'] == 'section')\n",
    "            total_sec = n_sec\n",
    "            for div in title['children']:\n",
    "                if div['type'] == 'division':\n",
    "                    total_sec += len(div.get('children', []))\n",
    "            print(f\"    {title['name']} ({total_sec} sections, {n_div} divisions)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create flat structured DataFrame with only main legal sections\n",
    "df_sections = df_main.copy()\n",
    "df_sections = df_sections.reset_index(drop=True)\n",
    "\n",
    "# Build hierarchy path (useful for PageIndex navigation)\n",
    "def build_path(r):\n",
    "    parts = []\n",
    "    if pd.notna(r['book_num']):\n",
    "        parts.append(f\"ภาค {int(r['book_num'])}: {r['book_name']}\")\n",
    "    if pd.notna(r['title_num']):\n",
    "        parts.append(f\"ลักษณะ {r['title_num']}: {r['title_name']}\")\n",
    "    if pd.notna(r['division_num']):\n",
    "        parts.append(f\"หมวด {int(r['division_num'])}: {r['division_name']}\")\n",
    "    parts.append(f\"มาตรา {r['section_num']}\")\n",
    "    return ' > '.join(parts)\n",
    "\n",
    "df_sections['hierarchy_path'] = df_sections.apply(build_path, axis=1)\n",
    "\n",
    "# Text length\n",
    "df_sections['text_length'] = df_sections['text_clean'].str.len()\n",
    "\n",
    "print(f'Total legal sections: {len(df_sections)}')\n",
    "print(f'Books: {df_sections[\"book_num\"].nunique()}')\n",
    "print(f'Titles: {df_sections[\"title_num\"].dropna().nunique()}')\n",
    "print(f'\\nSample hierarchy paths:')\n",
    "for path in df_sections['hierarchy_path'].sample(5, random_state=42):\n",
    "    print(f'  {path}')\n",
    "\n",
    "df_sections.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Thai Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "THAI_STOPWORDS = set(thai_stopwords())\n",
    "\n",
    "def preprocess_thai_text(text, tokenize_flag=True, remove_stopwords=False):\n",
    "    \"\"\"Preprocess Thai legal text: normalize, clean, and optionally tokenize.\"\"\"\n",
    "    if not text or str(text).strip() == '':\n",
    "        return [] if tokenize_flag else ''\n",
    "    \n",
    "    text = thai_normalize(str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if not tokenize_flag:\n",
    "        return text\n",
    "    \n",
    "    tokens = word_tokenize(text, engine='newmm')\n",
    "    tokens = [t.strip() for t in tokens if t.strip()]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in THAI_STOPWORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "df_sections['tokens'] = df_sections['section_text'].apply(\n",
    "    lambda x: preprocess_thai_text(x, tokenize_flag=True, remove_stopwords=False)\n",
    ")\n",
    "df_sections['token_count'] = df_sections['tokens'].apply(len)\n",
    "df_sections['text_normalized'] = df_sections['section_text'].apply(\n",
    "    lambda x: preprocess_thai_text(x, tokenize_flag=False)\n",
    ")\n",
    "\n",
    "print(f'Token count statistics:')\n",
    "print(df_sections['token_count'].describe())\n",
    "print(f'\\nExample tokenization (มาตรา {df_sections.iloc[0][\"section_num\"]}):')\n",
    "print(df_sections.iloc[0]['tokens'][:20])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entity Extraction (for Light Knowledge Graph)\n",
    "\n",
    "Extract legal entities that will form the nodes and edges of the Knowledge Graph:\n",
    "- **Penalties**: imprisonment (จำคุก), fines (ปรับ), death penalty (ประหารชีวิต)\n",
    "- **Cross-references**: sections referencing other sections\n",
    "- **Key legal terms**: offense types, actors, objects"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Entity extraction patterns\n",
    "PENALTY_PATTERNS = {\n",
    "    'imprisonment': re.compile(\n",
    "        r'จำคุก\\s*(?:ไม่เกิน\\s*)?'\n",
    "        r'(?:ตลอดชีวิต|'\n",
    "        r'(?:ตั้งแต่\\s*)?[๐-๙\\d]+\\s*(?:ปี|เดือน|วัน)'\n",
    "        r'(?:\\s*(?:ถึง|ไม่เกิน)\\s*[๐-๙\\d]+\\s*(?:ปี|เดือน|วัน))?)',\n",
    "        re.UNICODE\n",
    "    ),\n",
    "    'fine': re.compile(\n",
    "        r'ปรับ\\s*(?:ไม่เกิน\\s*)?'\n",
    "        r'(?:ตั้งแต่\\s*)?[๐-๙\\d,]+\\s*บาท'\n",
    "        r'(?:\\s*(?:ถึง|ไม่เกิน)\\s*[๐-๙\\d,]+\\s*บาท)?',\n",
    "        re.UNICODE\n",
    "    ),\n",
    "    'death_penalty': re.compile(r'ประหารชีวิต', re.UNICODE),\n",
    "    'detention': re.compile(r'กักขัง', re.UNICODE),\n",
    "    'confiscation': re.compile(r'ริบทรัพย์สิน', re.UNICODE),\n",
    "}\n",
    "\n",
    "CROSS_REF_PATTERN = re.compile(r'มาตรา\\s*([๐-๙\\d]+(?:/[๐-๙\\d]+)?)', re.UNICODE)\n",
    "\n",
    "LEGAL_TERMS = [\n",
    "    'กระทำความผิด', 'ผู้กระทำ', 'ผู้ใด', 'ต้องระวางโทษ', 'โทษ',\n",
    "    'จำเลย', 'โจทก์', 'พนักงานอัยการ', 'ศาล', 'พนักงานสอบสวน',\n",
    "    'ผู้เสียหาย', 'ทรัพย์สิน', 'สาธารณะ', 'รัฐ', 'เจ้าพนักงาน',\n",
    "    'อาวุธ', 'เอกสาร', 'ปลอม', 'ฉ้อโกง', 'ลักทรัพย์',\n",
    "    'ทำร้ายร่างกาย', 'ฆ่า', 'ข่มขืน', 'หมิ่นประมาท',\n",
    "    'วางเพลิง', 'ยาเสพติด',\n",
    "]\n",
    "\n",
    "print(f'Defined {len(PENALTY_PATTERNS)} penalty patterns')\n",
    "print(f'Defined {len(LEGAL_TERMS)} key legal terms')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_entities(text, section_num=None):\n",
    "    \"\"\"Extract legal entities from a section's text.\"\"\"\n",
    "    text_str = str(text)\n",
    "    entities = {}\n",
    "    \n",
    "    # 1. Penalties\n",
    "    penalties = []\n",
    "    if PENALTY_PATTERNS['death_penalty'].search(text_str):\n",
    "        penalties.append({'type': 'death_penalty'})\n",
    "    \n",
    "    imp_match = PENALTY_PATTERNS['imprisonment'].search(text_str)\n",
    "    if imp_match:\n",
    "        penalties.append({'type': 'imprisonment', 'detail': imp_match.group(0)})\n",
    "    elif 'จำคุก' in text_str:\n",
    "        penalties.append({'type': 'imprisonment', 'detail': 'จำคุก (unstructured)'})\n",
    "    \n",
    "    fine_match = PENALTY_PATTERNS['fine'].search(text_str)\n",
    "    if fine_match:\n",
    "        penalties.append({'type': 'fine', 'detail': fine_match.group(0)})\n",
    "    elif 'ปรับ' in text_str and 'ต้องระวางโทษ' in text_str:\n",
    "        penalties.append({'type': 'fine', 'detail': 'ปรับ (unstructured)'})\n",
    "    \n",
    "    if PENALTY_PATTERNS['detention'].search(text_str):\n",
    "        penalties.append({'type': 'detention'})\n",
    "    if PENALTY_PATTERNS['confiscation'].search(text_str):\n",
    "        penalties.append({'type': 'confiscation'})\n",
    "    \n",
    "    entities['penalties'] = penalties\n",
    "    \n",
    "    # 2. Cross-references (exclude self-reference)\n",
    "    cross_refs = CROSS_REF_PATTERN.findall(text_str)\n",
    "    cross_refs = list(set(thai_to_arabic(ref) for ref in cross_refs))\n",
    "    if section_num:\n",
    "        cross_refs = [ref for ref in cross_refs if ref != str(section_num)]\n",
    "    entities['cross_references'] = cross_refs\n",
    "    \n",
    "    # 3. Penalty combination type\n",
    "    has_imp = any(p['type'] == 'imprisonment' for p in penalties)\n",
    "    has_fine = any(p['type'] == 'fine' for p in penalties)\n",
    "    if 'ทั้งจำทั้งปรับ' in text_str:\n",
    "        entities['penalty_combination'] = 'both_mandatory'\n",
    "    elif 'หรือทั้งจำทั้งปรับ' in text_str or ('หรือปรับ' in text_str and has_imp):\n",
    "        entities['penalty_combination'] = 'both_optional'\n",
    "    elif has_imp and has_fine:\n",
    "        entities['penalty_combination'] = 'both'\n",
    "    elif has_imp:\n",
    "        entities['penalty_combination'] = 'imprisonment_only'\n",
    "    elif has_fine:\n",
    "        entities['penalty_combination'] = 'fine_only'\n",
    "    else:\n",
    "        entities['penalty_combination'] = 'none'\n",
    "    \n",
    "    # 4. Key legal terms present\n",
    "    entities['legal_terms'] = [term for term in LEGAL_TERMS if term in text_str]\n",
    "    \n",
    "    # 5. Condition flags\n",
    "    entities['has_exception'] = bool(re.search(r'เว้นแต่', text_str))\n",
    "    entities['has_condition'] = bool(re.search(r'ถ้า(?:หาก)?', text_str))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply entity extraction\n",
    "df_sections['entities'] = df_sections.apply(\n",
    "    lambda r: extract_entities(r['text_clean'], r['section_num']), axis=1\n",
    ")\n",
    "\n",
    "# Flatten key fields for analysis\n",
    "df_sections['has_imprisonment'] = df_sections['entities'].apply(\n",
    "    lambda e: any(p['type'] == 'imprisonment' for p in e['penalties']))\n",
    "df_sections['has_fine'] = df_sections['entities'].apply(\n",
    "    lambda e: any(p['type'] == 'fine' for p in e['penalties']))\n",
    "df_sections['has_death_penalty'] = df_sections['entities'].apply(\n",
    "    lambda e: any(p['type'] == 'death_penalty' for p in e['penalties']))\n",
    "df_sections['penalty_combination'] = df_sections['entities'].apply(\n",
    "    lambda e: e['penalty_combination'])\n",
    "df_sections['num_cross_refs'] = df_sections['entities'].apply(\n",
    "    lambda e: len(e['cross_references']))\n",
    "df_sections['num_legal_terms'] = df_sections['entities'].apply(\n",
    "    lambda e: len(e['legal_terms']))\n",
    "\n",
    "print('=== Entity Extraction Summary ===')\n",
    "print(f'Sections with imprisonment: {df_sections[\"has_imprisonment\"].sum()}')\n",
    "print(f'Sections with fines: {df_sections[\"has_fine\"].sum()}')\n",
    "print(f'Sections with death penalty: {df_sections[\"has_death_penalty\"].sum()}')\n",
    "print(f'\\nPenalty combination distribution:')\n",
    "print(df_sections['penalty_combination'].value_counts())\n",
    "print(f'\\nAverage cross-references per section: {df_sections[\"num_cross_refs\"].mean():.1f}')\n",
    "print(f'Total cross-references: {df_sections[\"num_cross_refs\"].sum()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build cross-reference graph (foundation for Light Knowledge Graph)\n",
    "def build_cross_reference_graph(df_sections):\n",
    "    \"\"\"Build a directed graph of cross-references between sections.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, row in df_sections.iterrows():\n",
    "        sec_num = str(row['section_num'])\n",
    "        \n",
    "        G.add_node(sec_num, **{\n",
    "            'book': str(row['book_name']) if row['book_name'] else '',\n",
    "            'title': str(row['title_name']) if row['title_name'] else '',\n",
    "            'division': str(row['division_name']) if row['division_name'] else '',\n",
    "            'text_preview': str(row['section_text'])[:100],\n",
    "            'has_penalty': bool(row['has_imprisonment'] or row['has_fine']),\n",
    "        })\n",
    "        \n",
    "        for ref in row['entities']['cross_references']:\n",
    "            if ref != sec_num:\n",
    "                G.add_edge(sec_num, ref, relation='references')\n",
    "    \n",
    "    return G\n",
    "\n",
    "G_refs = build_cross_reference_graph(df_sections)\n",
    "print(f'Cross-reference graph:')\n",
    "print(f'  Nodes: {G_refs.number_of_nodes()}')\n",
    "print(f'  Edges: {G_refs.number_of_edges()}')\n",
    "print(f'\\nMost referenced sections (top 10):')\n",
    "in_degrees = sorted(G_refs.in_degree(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, degree in in_degrees:\n",
    "    print(f'  มาตรา {node}: referenced by {degree} other sections')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. EDA Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Articles per Book\n",
    "book_counts = df_sections.groupby('book_name').size().sort_index()\n",
    "axes[0, 0].barh(range(len(book_counts)), book_counts.values, color='#3498db')\n",
    "axes[0, 0].set_yticks(range(len(book_counts)))\n",
    "axes[0, 0].set_yticklabels(book_counts.index, fontsize=9)\n",
    "axes[0, 0].set_xlabel('Number of Sections')\n",
    "axes[0, 0].set_title('Sections per Book')\n",
    "for i, v in enumerate(book_counts.values):\n",
    "    axes[0, 0].text(v + 1, i, str(v), va='center', fontsize=9)\n",
    "\n",
    "# 2. Text length distribution\n",
    "axes[0, 1].hist(df_sections['text_length'], bins=50, edgecolor='black', alpha=0.7, color='#2ecc71')\n",
    "axes[0, 1].set_xlabel('Text Length (characters)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Section Text Length')\n",
    "med = df_sections['text_length'].median()\n",
    "axes[0, 1].axvline(med, color='red', linestyle='--', label=f'Median: {med:.0f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Penalty type distribution\n",
    "penalty_data = {\n",
    "    'Imprisonment\\n(จำคุก)': df_sections['has_imprisonment'].sum(),\n",
    "    'Fine\\n(ปรับ)': df_sections['has_fine'].sum(),\n",
    "    'Death Penalty\\n(ประหารชีวิต)': df_sections['has_death_penalty'].sum(),\n",
    "    'No Penalty': (~(df_sections['has_imprisonment'] | df_sections['has_fine'] | df_sections['has_death_penalty'])).sum()\n",
    "}\n",
    "colors = ['#e74c3c', '#f39c12', '#2c3e50', '#95a5a6']\n",
    "bars = axes[1, 0].bar(penalty_data.keys(), penalty_data.values(), color=colors)\n",
    "axes[1, 0].set_ylabel('Number of Sections')\n",
    "axes[1, 0].set_title('Penalty Type Distribution')\n",
    "for bar, val in zip(bars, penalty_data.values()):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    str(val), ha='center', fontsize=9)\n",
    "\n",
    "# 4. Token count distribution\n",
    "axes[1, 1].hist(df_sections['token_count'], bins=50, edgecolor='black', alpha=0.7, color='#9b59b6')\n",
    "axes[1, 1].set_xlabel('Token Count')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Token Counts per Section')\n",
    "med_tok = df_sections['token_count'].median()\n",
    "axes[1, 1].axvline(med_tok, color='red', linestyle='--', label=f'Median: {med_tok:.0f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATA_DIR, 'eda_overview.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: data/eda_overview.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Penalty combination distribution\n",
    "combo_counts = df_sections['penalty_combination'].value_counts()\n",
    "combo_counts.plot(kind='bar', ax=axes[0], color='#e67e22', edgecolor='black', alpha=0.8)\n",
    "axes[0].set_title('Penalty Combination Types')\n",
    "axes[0].set_ylabel('Number of Sections')\n",
    "axes[0].set_xlabel('Combination Type')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for i, (idx, val) in enumerate(combo_counts.items()):\n",
    "    axes[0].text(i, val + 1, str(val), ha='center', fontsize=9)\n",
    "\n",
    "# 2. Cross-reference distribution\n",
    "max_refs = min(df_sections['num_cross_refs'].max() + 2, 20)\n",
    "axes[1].hist(df_sections['num_cross_refs'], bins=range(0, max_refs),\n",
    "             edgecolor='black', alpha=0.7, color='#1abc9c')\n",
    "axes[1].set_xlabel('Number of Cross-References')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Cross-Reference Count Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(DATA_DIR, 'eda_penalties_refs.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: data/eda_penalties_refs.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Most common legal terms\n",
    "all_terms = []\n",
    "for terms_list in df_sections['entities'].apply(lambda e: e['legal_terms']):\n",
    "    all_terms.extend(terms_list)\n",
    "\n",
    "term_counts = Counter(all_terms).most_common(20)\n",
    "\n",
    "if term_counts:\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    terms, counts = zip(*term_counts)\n",
    "    y_pos = range(len(terms))\n",
    "    ax.barh(y_pos, counts, color='#8e44ad', alpha=0.8)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(terms, fontsize=10)\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_title('Most Common Legal Terms in Thai Criminal Code')\n",
    "    ax.invert_yaxis()\n",
    "    for i, v in enumerate(counts):\n",
    "        ax.text(v + 0.5, i, str(v), va='center', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATA_DIR, 'eda_legal_terms.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print('Saved: data/eda_legal_terms.png')\n",
    "else:\n",
    "    print('No legal terms found to plot.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Preprocessed Data\n",
    "\n",
    "Export all artifacts for the next phases:\n",
    "- **PageIndex** will use `hierarchy_tree.json`\n",
    "- **Light Knowledge Graph** will use `entities.json` and `cross_reference_edges.json`\n",
    "- **Both** will use `preprocessed_criminal_code.csv`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "OUTPUT_DIR = os.path.join(DATA_DIR, 'preprocessed')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Main preprocessed CSV\n",
    "export_cols = [\n",
    "    'article_id', 'section_num', 'text_clean', 'section_text', 'text_normalized',\n",
    "    'notes', 'book_num', 'book_name', 'title_num', 'title_name',\n",
    "    'division_num', 'division_name', 'hierarchy_path', 'text_length', 'token_count',\n",
    "    'has_imprisonment', 'has_fine', 'has_death_penalty', 'penalty_combination',\n",
    "    'num_cross_refs', 'num_legal_terms'\n",
    "]\n",
    "df_export = df_sections[export_cols].copy()\n",
    "df_export.to_csv(os.path.join(OUTPUT_DIR, 'preprocessed_criminal_code.csv'), index=False)\n",
    "print(f'[1/6] Saved preprocessed_criminal_code.csv ({len(df_export)} rows)')\n",
    "\n",
    "# 2. Hierarchy tree JSON (for PageIndex)\n",
    "with open(os.path.join(OUTPUT_DIR, 'hierarchy_tree.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(hierarchy_tree, f, ensure_ascii=False, indent=2)\n",
    "print('[2/6] Saved hierarchy_tree.json')\n",
    "\n",
    "# 3. Entities JSON (for Light Knowledge Graph)\n",
    "entities_export = []\n",
    "for _, row in df_sections.iterrows():\n",
    "    entities_export.append({\n",
    "        'section_num': row['section_num'],\n",
    "        'hierarchy_path': row['hierarchy_path'],\n",
    "        'entities': row['entities']\n",
    "    })\n",
    "with open(os.path.join(OUTPUT_DIR, 'entities.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(entities_export, f, ensure_ascii=False, indent=2)\n",
    "print(f'[3/6] Saved entities.json ({len(entities_export)} sections)')\n",
    "\n",
    "# 4. Cross-reference graph (GEXF format)\n",
    "nx.write_gexf(G_refs, os.path.join(OUTPUT_DIR, 'cross_references.gexf'))\n",
    "print(f'[4/6] Saved cross_references.gexf ({G_refs.number_of_nodes()} nodes, {G_refs.number_of_edges()} edges)')\n",
    "\n",
    "# 5. Cross-reference edge list JSON\n",
    "edges_data = [\n",
    "    {'source': u, 'target': v, 'relation': d['relation']}\n",
    "    for u, v, d in G_refs.edges(data=True)\n",
    "]\n",
    "with open(os.path.join(OUTPUT_DIR, 'cross_reference_edges.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump(edges_data, f, ensure_ascii=False, indent=2)\n",
    "print(f'[5/6] Saved cross_reference_edges.json ({len(edges_data)} edges)')\n",
    "\n",
    "# 6. Full parsed DataFrame (including intro sections)\n",
    "df_parsed.to_csv(os.path.join(OUTPUT_DIR, 'full_parsed_with_headers.csv'), index=False)\n",
    "print(f'[6/6] Saved full_parsed_with_headers.csv ({len(df_parsed)} rows)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Final summary\n",
    "print('=' * 60)\n",
    "print('DATA COLLECTION & PREPROCESSING COMPLETE')\n",
    "print('=' * 60)\n",
    "print(f'''\n",
    "Dataset Summary:\n",
    "  Raw CSV rows:                {len(df_raw)}\n",
    "  Legal sections (มาตรา):       {len(df_sections)}\n",
    "  Books (ภาค):                  {df_sections['book_num'].nunique()}\n",
    "  Titles (ลักษณะ):              {df_sections['title_num'].dropna().nunique()}\n",
    "  Divisions (หมวด):             {df_sections['division_num'].dropna().nunique()}\n",
    "\n",
    "Entity Extraction:\n",
    "  With imprisonment penalty:   {df_sections['has_imprisonment'].sum()}\n",
    "  With fine penalty:           {df_sections['has_fine'].sum()}\n",
    "  With death penalty:          {df_sections['has_death_penalty'].sum()}\n",
    "  Total cross-references:      {df_sections['num_cross_refs'].sum()}\n",
    "  Cross-reference graph:       {G_refs.number_of_nodes()} nodes, {G_refs.number_of_edges()} edges\n",
    "\n",
    "Output Files (in data/preprocessed/):\n",
    "  preprocessed_criminal_code.csv  -> Main dataset for PageIndex + KG\n",
    "  hierarchy_tree.json             -> Tree structure for PageIndex\n",
    "  entities.json                   -> Entity data for Light KG\n",
    "  cross_references.gexf           -> Cross-reference graph (GEXF)\n",
    "  cross_reference_edges.json      -> Edge list for KG\n",
    "  full_parsed_with_headers.csv    -> Complete parsed data\n",
    "\n",
    "Next Steps (Task 3 - Model Development):\n",
    "  1. Build PageIndex tree from hierarchy_tree.json\n",
    "  2. Construct Light Knowledge Graph from entities.json + edges\n",
    "  3. Implement retrieval pipeline for Q&A\n",
    "''')\n",
    "\n",
    "# List output files with sizes\n",
    "print('Output file sizes:')\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(fpath)\n",
    "    print(f'  {f}: {size / 1024:.1f} KB')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n---\n# Task 3: Model Development\n\nEverything below uses the preprocessed data from Task 2 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Model Development\n",
    "\n",
    "## Comparing PageIndex vs PageIndex + Light Knowledge Graph for Thai Criminal Law Q&A Chatbot\n",
    "\n",
    "**Course:** CS652 Applied Machine Learning  \n",
    "**Student:** 6809036087  \n",
    "**LLM:** Qwen3:8b via Ollama (local inference)\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Query → Pipeline A (PageIndex Only)    → Retrieved Sections → Qwen3 → Answer\n",
    "Query → Pipeline B (PageIndex + KG)    → Retrieved Sections → Qwen3 → Answer\n",
    "```\n",
    "\n",
    "### Notebook Sections\n",
    "1. Setup & Dependencies\n",
    "2. Build PageIndex Tree Structure\n",
    "3. Generate Node Summaries (Bottom-Up with Qwen3)\n",
    "4. Build Light Knowledge Graph\n",
    "5. Pipeline A: PageIndex Retrieval\n",
    "6. Pipeline B: PageIndex + KG Retrieval\n",
    "7. Evaluation Framework\n",
    "8. Run Evaluation & Compare\n",
    "9. Analysis & Discussion\n",
    "10. Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import ollama\n",
    "\n",
    "# Configure matplotlib for Thai text\n",
    "thai_fonts = [f.name for f in matplotlib.font_manager.fontManager.ttflist \n",
    "              if 'Tahoma' in f.name or 'Sarabun' in f.name or 'Garuda' in f.name \n",
    "              or 'Loma' in f.name or 'TH ' in f.name or 'Angsana' in f.name\n",
    "              or 'Browallia' in f.name or 'Cordia' in f.name]\n",
    "\n",
    "if thai_fonts:\n",
    "    plt.rcParams['font.family'] = thai_fonts[0]\n",
    "    print(f\"Using Thai font: {thai_fonts[0]}\")\n",
    "else:\n",
    "    print(\"No Thai font found, using default font\")\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"All imports loaded successfully\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify LLM model is available\n# Auto-detect: prefer qwen3, fall back to llama3.2\nmodels = ollama.list()\nmodel_names = [m.model for m in models.models]\nprint(\"Available Ollama models:\", model_names)\n\nMODEL_NAME = None\nfor candidate in [\"qwen3:8b\", \"qwen3:8b\", \"qwen3:latest\", \"llama3.2:latest\", \"llama3.2:1b\"]:\n    if any(candidate in name for name in model_names):\n        MODEL_NAME = candidate\n        break\n\nif MODEL_NAME is None:\n    print(\"\\n⚠ No supported model found! Please run one of:\")\n    print(\"  ollama pull qwen3:8b     (recommended, ~5 GB)\")\n    print(\"  ollama pull qwen3:8b     (smaller, ~2.5 GB)\")\n    print(\"  ollama pull llama3.2     (fallback, ~2 GB)\")\n    raise RuntimeError(\"No supported model available\")\n\nprint(f\"\\n✓ Using model: {MODEL_NAME}\")\nif \"llama\" in MODEL_NAME:\n    print(\"  Note: Using Llama 3.2. Thai support may be limited compared to Qwen3.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load all preprocessed data from Task 2\n",
    "DATA_DIR = Path(\"data/preprocessed\")\n",
    "RESULTS_DIR = Path(\"data/results\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(DATA_DIR / \"hierarchy_tree.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hierarchy_tree = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / \"entities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entities_data = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / \"cross_reference_edges.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cross_ref_edges = json.load(f)\n",
    "\n",
    "with open(DATA_DIR / \"qa_test_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / \"preprocessed_criminal_code.csv\")\n",
    "\n",
    "print(f\"Hierarchy tree: root with {len(hierarchy_tree['children'])} books\")\n",
    "print(f\"Entities: {len(entities_data)} sections\")\n",
    "print(f\"Cross-reference edges: {len(cross_ref_edges)} edges\")\n",
    "print(f\"Q&A dataset: {len(qa_dataset)} questions\")\n",
    "print(f\"Preprocessed CSV: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Helper function to call Qwen3\n",
    "def ask_qwen3(prompt, temperature=0.3, max_retries=3):\n",
    "    \"\"\"Call Qwen3 via Ollama with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": temperature, \"num_predict\": 1024}\n",
    "            )\n",
    "            text = response[\"message\"][\"content\"]\n",
    "            # Remove <think>...</think> blocks if present (Qwen3 thinking mode)\n",
    "            text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(f\"Error calling Qwen3: {e}\")\n",
    "                return \"\"\n",
    "\n",
    "# Quick test\n",
    "test_response = ask_qwen3(\"สวัสดีครับ ตอบสั้นๆ ว่าคุณคือใคร\")\n",
    "print(\"Test response:\", test_response[:200])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Build PageIndex Tree Structure\n",
    "\n",
    "Convert the hierarchy JSON into a navigable tree with parent/child links and unique node IDs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TreeNode:\n",
    "    \"\"\"A node in the PageIndex tree.\"\"\"\n",
    "    def __init__(self, node_id, level, name, text=\"\", parent=None):\n",
    "        self.node_id = node_id\n",
    "        self.level = level        # root, book, title, division, section\n",
    "        self.name = name\n",
    "        self.text = text           # full legal text (sections only)\n",
    "        self.summary = \"\"          # LLM-generated summary\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.section_num = None    # for leaf nodes\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"TreeNode({self.node_id}, {self.level}, {self.name[:40]}...)\"\n",
    "\n",
    "\n",
    "def build_tree(tree_data):\n",
    "    \"\"\"Build navigable tree from hierarchy_tree.json.\"\"\"\n",
    "    nodes = {}  # node_id -> TreeNode\n",
    "    \n",
    "    # Create root\n",
    "    root = TreeNode(\"root\", \"root\", tree_data[\"root\"])\n",
    "    nodes[\"root\"] = root\n",
    "    \n",
    "    def add_children(parent_node, children_data):\n",
    "        for child_data in children_data:\n",
    "            child_type = child_data[\"type\"]\n",
    "            \n",
    "            if child_type == \"section\":\n",
    "                node_id = f\"section_{child_data['number']}\"\n",
    "                name = f\"มาตรา {child_data['number']}\"\n",
    "                text = child_data.get(\"full_text\", child_data.get(\"text\", \"\"))\n",
    "                node = TreeNode(node_id, \"section\", name, text, parent_node)\n",
    "                node.section_num = str(child_data[\"number\"])\n",
    "            else:\n",
    "                num = child_data.get(\"number\", \"\")\n",
    "                name = child_data.get(\"name\", f\"{child_type} {num}\")\n",
    "                node_id = f\"{child_type}_{num}\"\n",
    "                node = TreeNode(node_id, child_type, name, parent=parent_node)\n",
    "            \n",
    "            nodes[node_id] = node\n",
    "            parent_node.children.append(node)\n",
    "            \n",
    "            if \"children\" in child_data and child_data[\"children\"]:\n",
    "                add_children(node, child_data[\"children\"])\n",
    "    \n",
    "    add_children(root, tree_data[\"children\"])\n",
    "    return root, nodes\n",
    "\n",
    "\n",
    "root, all_nodes = build_tree(hierarchy_tree)\n",
    "\n",
    "# Count nodes by level\n",
    "level_counts = Counter(n.level for n in all_nodes.values())\n",
    "print(\"Tree node counts by level:\")\n",
    "for level, count in sorted(level_counts.items()):\n",
    "    print(f\"  {level}: {count}\")\n",
    "print(f\"\\nTotal nodes: {len(all_nodes)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify tree structure by printing top 2 levels\n",
    "print(f\"Root: {root.name}\")\n",
    "for book in root.children:\n",
    "    print(f\"  ├── {book.name} ({len(book.children)} titles)\")\n",
    "    for title in book.children[:3]:\n",
    "        n_div = len(title.children)\n",
    "        n_sec = sum(len(d.children) for d in title.children if d.level == 'division')\n",
    "        # Some titles have sections directly (no division)\n",
    "        n_sec += sum(1 for c in title.children if c.level == 'section')\n",
    "        print(f\"  │   ├── {title.name} ({n_div} children, ~{n_sec} sections)\")\n",
    "    if len(book.children) > 3:\n",
    "        print(f\"  │   └── ... and {len(book.children) - 3} more titles\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Generate Node Summaries (Bottom-Up with Qwen3)\n",
    "\n",
    "Generate Thai summaries for each node, starting from leaf sections and building up. Summaries are cached to avoid re-generation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SUMMARIES_PATH = DATA_DIR / \"node_summaries.json\"\n",
    "\n",
    "# Load cached summaries if they exist\n",
    "if SUMMARIES_PATH.exists():\n",
    "    with open(SUMMARIES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        cached_summaries = json.load(f)\n",
    "    print(f\"Loaded {len(cached_summaries)} cached summaries\")\n",
    "else:\n",
    "    cached_summaries = {}\n",
    "    print(\"No cached summaries found, will generate from scratch\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def summarize_section(node):\n",
    "    \"\"\"Summarize a leaf section node.\"\"\"\n",
    "    prompt = f\"\"\"สรุปมาตรากฎหมายต่อไปนี้ให้สั้นกระชับ 1-2 ประโยค เป็นภาษาไทย ระบุสาระสำคัญและบทลงโทษ (ถ้ามี):\n",
    "\n",
    "{node.text[:1500]}\n",
    "\n",
    "สรุป:\"\"\"\n",
    "    return ask_qwen3(prompt, temperature=0.2)\n",
    "\n",
    "\n",
    "def summarize_parent(node, children_summaries):\n",
    "    \"\"\"Summarize a parent node from its children's summaries.\"\"\"\n",
    "    summaries_text = \"\\n\".join([\n",
    "        f\"- {child.name}: {summary}\" \n",
    "        for child, summary in children_summaries\n",
    "    ])\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(summaries_text) > 3000:\n",
    "        summaries_text = summaries_text[:3000] + \"\\n... (ตัดทอน)\"\n",
    "    \n",
    "    prompt = f\"\"\"สรุปภาพรวมของ \"{node.name}\" จากเนื้อหาย่อยต่อไปนี้ ให้สั้นกระชับ 2-3 ประโยค เป็นภาษาไทย:\n",
    "\n",
    "{summaries_text}\n",
    "\n",
    "สรุปภาพรวม:\"\"\"\n",
    "    return ask_qwen3(prompt, temperature=0.2)\n",
    "\n",
    "\n",
    "def generate_all_summaries(root_node, nodes_dict, cached):\n",
    "    \"\"\"Generate summaries bottom-up for all nodes.\"\"\"\n",
    "    total = len(nodes_dict)\n",
    "    generated = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    def process_node(node):\n",
    "        nonlocal generated, skipped\n",
    "        \n",
    "        # Check cache first\n",
    "        if node.node_id in cached:\n",
    "            node.summary = cached[node.node_id]\n",
    "            skipped += 1\n",
    "            # Still need to process children for their summaries\n",
    "            for child in node.children:\n",
    "                process_node(child)\n",
    "            return node.summary\n",
    "        \n",
    "        # Process children first (bottom-up)\n",
    "        children_summaries = []\n",
    "        for child in node.children:\n",
    "            child_summary = process_node(child)\n",
    "            children_summaries.append((child, child_summary))\n",
    "        \n",
    "        # Generate summary\n",
    "        if node.level == \"section\":\n",
    "            node.summary = summarize_section(node)\n",
    "        elif node.level == \"root\":\n",
    "            node.summary = summarize_parent(node, children_summaries)\n",
    "        elif children_summaries:\n",
    "            node.summary = summarize_parent(node, children_summaries)\n",
    "        else:\n",
    "            node.summary = node.name\n",
    "        \n",
    "        # Cache immediately\n",
    "        cached[node.node_id] = node.summary\n",
    "        generated += 1\n",
    "        \n",
    "        if generated % 20 == 0:\n",
    "            print(f\"  Generated {generated}/{total - skipped} summaries...\")\n",
    "            # Save intermediate cache\n",
    "            with open(SUMMARIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(cached, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return node.summary\n",
    "    \n",
    "    process_node(root_node)\n",
    "    \n",
    "    # Final save\n",
    "    with open(SUMMARIES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cached, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nDone! Generated: {generated}, Cached: {skipped}, Total: {total}\")\n",
    "    return cached"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate summaries (this takes ~30-60 minutes on first run)\n",
    "print(\"Generating node summaries (bottom-up)...\")\n",
    "print(\"This may take a while on first run. Summaries are cached for future use.\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "cached_summaries = generate_all_summaries(root, all_nodes, cached_summaries)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTime elapsed: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Summaries saved to: {SUMMARIES_PATH}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show sample summaries at each level\n",
    "print(\"=== Sample Summaries ===\")\n",
    "print(f\"\\n[Root] {root.name}:\")\n",
    "print(f\"  {root.summary[:200]}\")\n",
    "\n",
    "if root.children:\n",
    "    book = root.children[0]\n",
    "    print(f\"\\n[Book] {book.name}:\")\n",
    "    print(f\"  {book.summary[:200]}\")\n",
    "    \n",
    "    if book.children:\n",
    "        title = book.children[0]\n",
    "        print(f\"\\n[Title] {title.name}:\")\n",
    "        print(f\"  {title.summary[:200]}\")\n",
    "        \n",
    "        if title.children:\n",
    "            div_or_sec = title.children[0]\n",
    "            print(f\"\\n[{div_or_sec.level.title()}] {div_or_sec.name}:\")\n",
    "            print(f\"  {div_or_sec.summary[:200]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Build Light Knowledge Graph\n",
    "\n",
    "Create a NetworkX directed graph with section nodes, cross-reference edges, and entity attributes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build Knowledge Graph\n",
    "kg = nx.DiGraph()\n",
    "\n",
    "# Build section lookup from DataFrame\n",
    "section_texts = {}\n",
    "for _, row in df.iterrows():\n",
    "    sec_num = str(row['section_num'])\n",
    "    section_texts[sec_num] = row.get('section_text', row.get('text_clean', ''))\n",
    "\n",
    "# Add nodes from entities data\n",
    "for entity in entities_data:\n",
    "    sec_num = str(entity[\"section_num\"])\n",
    "    kg.add_node(sec_num, \n",
    "                hierarchy_path=entity[\"hierarchy_path\"],\n",
    "                penalties=[p[\"type\"] for p in entity[\"entities\"][\"penalties\"]],\n",
    "                penalty_combination=entity[\"entities\"][\"penalty_combination\"],\n",
    "                legal_terms=entity[\"entities\"][\"legal_terms\"],\n",
    "                has_exception=entity[\"entities\"][\"has_exception\"],\n",
    "                has_condition=entity[\"entities\"][\"has_condition\"],\n",
    "                text=section_texts.get(sec_num, \"\"))\n",
    "\n",
    "# Add cross-reference edges\n",
    "for edge in cross_ref_edges:\n",
    "    src = str(edge[\"source\"])\n",
    "    tgt = str(edge[\"target\"])\n",
    "    if src in kg.nodes and tgt in kg.nodes:\n",
    "        kg.add_edge(src, tgt, relation=\"references\")\n",
    "\n",
    "print(f\"Knowledge Graph:\")\n",
    "print(f\"  Nodes: {kg.number_of_nodes()}\")\n",
    "print(f\"  Edges (cross-references): {kg.number_of_edges()}\")\n",
    "print(f\"  Average out-degree: {sum(d for _, d in kg.out_degree()) / kg.number_of_nodes():.2f}\")\n",
    "print(f\"  Nodes with cross-refs: {sum(1 for _, d in kg.out_degree() if d > 0)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build division membership lookup for thematic expansion\n",
    "section_to_division = {}\n",
    "division_sections = defaultdict(list)\n",
    "\n",
    "for entity in entities_data:\n",
    "    sec_num = str(entity[\"section_num\"])\n",
    "    path = entity[\"hierarchy_path\"]\n",
    "    # Extract division from hierarchy path\n",
    "    parts = path.split(\" > \")\n",
    "    if len(parts) >= 3:\n",
    "        division = parts[2]  # e.g., \"หมวด 1: บทนิยาม\"\n",
    "    else:\n",
    "        division = parts[-1]\n",
    "    section_to_division[sec_num] = division\n",
    "    division_sections[division].append(sec_num)\n",
    "\n",
    "print(f\"Division groups: {len(division_sections)}\")\n",
    "print(f\"Average sections per division: {np.mean([len(v) for v in division_sections.values()]):.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# KG query functions\n",
    "\n",
    "def get_cross_refs(section_num, hops=1):\n",
    "    \"\"\"Get cross-referenced sections (1-hop neighbors).\"\"\"\n",
    "    section_num = str(section_num)\n",
    "    if section_num not in kg:\n",
    "        return []\n",
    "    \n",
    "    refs = set()\n",
    "    current = {section_num}\n",
    "    for _ in range(hops):\n",
    "        next_hop = set()\n",
    "        for node in current:\n",
    "            # Outgoing references\n",
    "            next_hop.update(kg.successors(node))\n",
    "            # Incoming references\n",
    "            next_hop.update(kg.predecessors(node))\n",
    "        refs.update(next_hop)\n",
    "        current = next_hop\n",
    "    \n",
    "    refs.discard(section_num)\n",
    "    return list(refs)\n",
    "\n",
    "\n",
    "def get_penalty_info(section_num):\n",
    "    \"\"\"Get penalty information for a section.\"\"\"\n",
    "    section_num = str(section_num)\n",
    "    if section_num not in kg:\n",
    "        return {}\n",
    "    data = kg.nodes[section_num]\n",
    "    return {\n",
    "        \"penalties\": data.get(\"penalties\", []),\n",
    "        \"combination\": data.get(\"penalty_combination\", \"none\")\n",
    "    }\n",
    "\n",
    "\n",
    "def get_same_division_sections(section_num, max_sections=5):\n",
    "    \"\"\"Get other sections in the same division.\"\"\"\n",
    "    section_num = str(section_num)\n",
    "    division = section_to_division.get(section_num)\n",
    "    if not division:\n",
    "        return []\n",
    "    siblings = [s for s in division_sections[division] if s != section_num]\n",
    "    return siblings[:max_sections]\n",
    "\n",
    "\n",
    "# Test KG functions\n",
    "test_sec = \"334\"  # ลักทรัพย์ (theft)\n",
    "print(f\"Test section: มาตรา {test_sec}\")\n",
    "print(f\"  Cross-refs: {get_cross_refs(test_sec)}\")\n",
    "print(f\"  Penalties: {get_penalty_info(test_sec)}\")\n",
    "print(f\"  Same division: {get_same_division_sections(test_sec)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Pipeline A: PageIndex Retrieval\n",
    "\n",
    "Tree-based retrieval: Start at root, ask LLM which branch is most relevant at each level (beam width = 2), descend to leaf sections."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def pageindex_retrieve(query, root_node, beam_width=2, verbose=False):\n",
    "    \"\"\"\n",
    "    PageIndex tree traversal retrieval.\n",
    "    At each level, ask Qwen3 to select the top-k most relevant branches.\n",
    "    Returns list of retrieved section TreeNodes.\n",
    "    \"\"\"\n",
    "    current_nodes = [root_node]\n",
    "    retrieved_sections = []\n",
    "    \n",
    "    while current_nodes:\n",
    "        next_level_nodes = []\n",
    "        \n",
    "        for node in current_nodes:\n",
    "            if not node.children:\n",
    "                # Leaf node (section) - add to results\n",
    "                if node.level == \"section\":\n",
    "                    retrieved_sections.append(node)\n",
    "                continue\n",
    "            \n",
    "            # If all children are sections, include all (already at leaf level)\n",
    "            if all(c.level == \"section\" for c in node.children):\n",
    "                # Ask LLM to pick most relevant sections\n",
    "                choices = \"\\n\".join([\n",
    "                    f\"{i+1}. {c.name}: {c.summary[:150]}\" \n",
    "                    for i, c in enumerate(node.children)\n",
    "                ])\n",
    "                \n",
    "                prompt = f\"\"\"คำถาม: {query}\n",
    "\n",
    "เลือกมาตราที่เกี่ยวข้องกับคำถามมากที่สุด (เลือกได้หลายข้อ ไม่เกิน {min(beam_width + 1, len(node.children))} ข้อ)\n",
    "ตอบเป็นตัวเลขคั่นด้วยเครื่องหมายจุลภาค เช่น 1,3,5\n",
    "\n",
    "ตัวเลือก:\n",
    "{choices}\n",
    "\n",
    "ตอบเฉพาะตัวเลข:\"\"\"\n",
    "                \n",
    "                response = ask_qwen3(prompt, temperature=0.1)\n",
    "                selected = parse_selection(response, len(node.children))\n",
    "                \n",
    "                if not selected:\n",
    "                    selected = list(range(min(beam_width, len(node.children))))\n",
    "                \n",
    "                for idx in selected[:beam_width + 1]:\n",
    "                    retrieved_sections.append(node.children[idx])\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  [{node.name}] Selected sections: {[node.children[i].name for i in selected[:beam_width+1]]}\")\n",
    "                continue\n",
    "            \n",
    "            # Non-leaf: ask LLM to choose branches\n",
    "            choices = \"\\n\".join([\n",
    "                f\"{i+1}. {c.name}: {c.summary[:150]}\" \n",
    "                for i, c in enumerate(node.children)\n",
    "            ])\n",
    "            \n",
    "            prompt = f\"\"\"คำถาม: {query}\n",
    "\n",
    "เลือกหัวข้อที่เกี่ยวข้องกับคำถามมากที่สุด (เลือกได้ {min(beam_width, len(node.children))} ข้อ)\n",
    "ตอบเป็นตัวเลขคั่นด้วยเครื่องหมายจุลภาค เช่น 1,3\n",
    "\n",
    "ตัวเลือก:\n",
    "{choices}\n",
    "\n",
    "ตอบเฉพาะตัวเลข:\"\"\"\n",
    "            \n",
    "            response = ask_qwen3(prompt, temperature=0.1)\n",
    "            selected = parse_selection(response, len(node.children))\n",
    "            \n",
    "            if not selected:\n",
    "                selected = list(range(min(beam_width, len(node.children))))\n",
    "            \n",
    "            for idx in selected[:beam_width]:\n",
    "                next_level_nodes.append(node.children[idx])\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  [{node.level}] {node.name} -> Selected: {[node.children[i].name for i in selected[:beam_width]]}\")\n",
    "        \n",
    "        current_nodes = next_level_nodes\n",
    "    \n",
    "    return retrieved_sections\n",
    "\n",
    "\n",
    "def parse_selection(response, max_options):\n",
    "    \"\"\"Parse LLM selection response into list of 0-based indices.\"\"\"\n",
    "    numbers = re.findall(r'\\d+', response)\n",
    "    indices = []\n",
    "    for n in numbers:\n",
    "        idx = int(n) - 1  # Convert to 0-based\n",
    "        if 0 <= idx < max_options:\n",
    "            indices.append(idx)\n",
    "    return list(dict.fromkeys(indices))  # Deduplicate preserving order"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_answer(query, retrieved_sections, pipeline_name=\"\"):\n",
    "    \"\"\"Generate answer from retrieved sections using Qwen3.\"\"\"\n",
    "    # Build context from retrieved sections\n",
    "    context_parts = []\n",
    "    for node in retrieved_sections:\n",
    "        text = node.text if node.text else node.summary\n",
    "        context_parts.append(f\"{node.name}: {text[:500]}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Truncate context if too long\n",
    "    if len(context) > 4000:\n",
    "        context = context[:4000] + \"\\n... (ตัดทอน)\"\n",
    "    \n",
    "    prompt = f\"\"\"คุณเป็นผู้เชี่ยวชาญด้านกฎหมายอาญาไทย ตอบคำถามต่อไปนี้โดยอ้างอิงจากประมวลกฎหมายอาญาที่ให้มา\n",
    "ตอบเป็นภาษาไทย ให้ครบถ้วนและถูกต้อง อ้างอิงเลขมาตราที่เกี่ยวข้อง\n",
    "\n",
    "=== บทบัญญัติที่เกี่ยวข้อง ===\n",
    "{context}\n",
    "\n",
    "=== คำถาม ===\n",
    "{query}\n",
    "\n",
    "=== คำตอบ ===\"\"\"\n",
    "    \n",
    "    return ask_qwen3(prompt, temperature=0.3)\n",
    "\n",
    "\n",
    "def pipeline_a(query, verbose=False):\n",
    "    \"\"\"Full Pipeline A: PageIndex retrieval + answer generation.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Retrieve\n",
    "    retrieved = pageindex_retrieve(query, root, beam_width=2, verbose=verbose)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = generate_answer(query, retrieved, \"Pipeline A\")\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_sections\": [n.section_num for n in retrieved if n.section_num],\n",
    "        \"retrieved_names\": [n.name for n in retrieved],\n",
    "        \"latency\": elapsed\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test Pipeline A with a sample question\n",
    "test_query = qa_dataset[0][\"question\"]  # ความผิดฐานลักทรัพย์มีโทษอย่างไร?\n",
    "print(f\"Test query: {test_query}\")\n",
    "print(f\"Expected sections: {qa_dataset[0]['reference_sections']}\")\n",
    "print()\n",
    "\n",
    "result_a = pipeline_a(test_query, verbose=True)\n",
    "print(f\"\\nRetrieved sections: {result_a['retrieved_sections']}\")\n",
    "print(f\"Latency: {result_a['latency']:.1f}s\")\n",
    "print(f\"\\nAnswer: {result_a['answer'][:500]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Pipeline B: PageIndex + KG Retrieval\n",
    "\n",
    "Same PageIndex traversal, then expand retrieved sections using Knowledge Graph (cross-references, same-division siblings)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def kg_expand(retrieved_sections, max_expansion=5):\n",
    "    \"\"\"\n",
    "    Expand retrieved sections using Knowledge Graph.\n",
    "    - Follow cross-reference edges (1-hop)\n",
    "    - Add same-division siblings for context\n",
    "    Returns expanded list of section numbers.\n",
    "    \"\"\"\n",
    "    original_sections = set(n.section_num for n in retrieved_sections if n.section_num)\n",
    "    expanded = set(original_sections)\n",
    "    \n",
    "    for sec_num in original_sections:\n",
    "        # Cross-reference expansion\n",
    "        cross_refs = get_cross_refs(sec_num, hops=1)\n",
    "        expanded.update(cross_refs[:3])  # Limit per section\n",
    "        \n",
    "        # Same-division siblings (for comparison questions)\n",
    "        siblings = get_same_division_sections(sec_num, max_sections=2)\n",
    "        expanded.update(siblings)\n",
    "    \n",
    "    # Limit total expansion\n",
    "    expanded = list(expanded)\n",
    "    if len(expanded) > len(original_sections) + max_expansion:\n",
    "        # Keep originals + top expansion by relevance (degree in KG)\n",
    "        new_sections = [s for s in expanded if s not in original_sections]\n",
    "        new_sections.sort(key=lambda s: kg.degree(s) if s in kg else 0, reverse=True)\n",
    "        expanded = list(original_sections) + new_sections[:max_expansion]\n",
    "    \n",
    "    return expanded\n",
    "\n",
    "\n",
    "def sections_to_nodes(section_nums):\n",
    "    \"\"\"Convert section numbers to TreeNodes (or create minimal nodes).\"\"\"\n",
    "    nodes = []\n",
    "    for sec_num in section_nums:\n",
    "        node_id = f\"section_{sec_num}\"\n",
    "        if node_id in all_nodes:\n",
    "            nodes.append(all_nodes[node_id])\n",
    "        elif sec_num in kg:\n",
    "            # Create a minimal node from KG data\n",
    "            node = TreeNode(node_id, \"section\", f\"มาตรา {sec_num}\", \n",
    "                          kg.nodes[sec_num].get(\"text\", \"\"))\n",
    "            node.section_num = sec_num\n",
    "            nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def pipeline_b(query, verbose=False):\n",
    "    \"\"\"Full Pipeline B: PageIndex + KG retrieval + answer generation.\"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    # Step 1: PageIndex retrieval (same as Pipeline A)\n",
    "    retrieved = pageindex_retrieve(query, root, beam_width=2, verbose=verbose)\n",
    "    \n",
    "    # Step 2: KG expansion\n",
    "    expanded_sections = kg_expand(retrieved, max_expansion=5)\n",
    "    \n",
    "    if verbose:\n",
    "        original = set(n.section_num for n in retrieved if n.section_num)\n",
    "        new_from_kg = set(expanded_sections) - original\n",
    "        print(f\"  [KG] Original: {original}, Added: {new_from_kg}\")\n",
    "    \n",
    "    # Step 3: Convert back to nodes for answer generation\n",
    "    expanded_nodes = sections_to_nodes(expanded_sections)\n",
    "    \n",
    "    # Step 4: Generate answer with expanded context\n",
    "    answer = generate_answer(query, expanded_nodes, \"Pipeline B\")\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_sections\": expanded_sections,\n",
    "        \"original_sections\": [n.section_num for n in retrieved if n.section_num],\n",
    "        \"kg_added_sections\": list(set(expanded_sections) - set(n.section_num for n in retrieved if n.section_num)),\n",
    "        \"retrieved_names\": [f\"มาตรา {s}\" for s in expanded_sections],\n",
    "        \"latency\": elapsed\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test Pipeline B with the same question\n",
    "print(f\"Test query: {test_query}\")\n",
    "print(f\"Expected sections: {qa_dataset[0]['reference_sections']}\")\n",
    "print()\n",
    "\n",
    "result_b = pipeline_b(test_query, verbose=True)\n",
    "print(f\"\\nRetrieved sections (total): {result_b['retrieved_sections']}\")\n",
    "print(f\"  Original from PageIndex: {result_b['original_sections']}\")\n",
    "print(f\"  Added by KG: {result_b['kg_added_sections']}\")\n",
    "print(f\"Latency: {result_b['latency']:.1f}s\")\n",
    "print(f\"\\nAnswer: {result_b['answer'][:500]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Evaluation Framework\n",
    "\n",
    "Evaluate both pipelines on 20 test questions. Metrics: retrieval recall/precision, answer quality (LLM-as-judge), latency."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_retrieval_metrics(retrieved_sections, ground_truth_sections):\n",
    "    \"\"\"Compute retrieval precision and recall.\"\"\"\n",
    "    retrieved = set(str(s) for s in retrieved_sections)\n",
    "    ground_truth = set(str(s) for s in ground_truth_sections)\n",
    "    \n",
    "    if not ground_truth:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    true_positives = retrieved & ground_truth\n",
    "    \n",
    "    precision = len(true_positives) / len(retrieved) if retrieved else 0.0\n",
    "    recall = len(true_positives) / len(ground_truth) if ground_truth else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def llm_judge_answer(question, generated_answer, ground_truth_answer):\n",
    "    \"\"\"Use Qwen3 as a judge to score answer quality (1-5).\"\"\"\n",
    "    prompt = f\"\"\"ให้คะแนนคำตอบต่อไปนี้ เทียบกับคำตอบที่ถูกต้อง (1-5 คะแนน)\n",
    "\n",
    "เกณฑ์การให้คะแนน:\n",
    "5 = ถูกต้องครบถ้วน ตรงกับคำตอบที่ถูกต้อง\n",
    "4 = ถูกต้องเป็นส่วนใหญ่ ขาดรายละเอียดเล็กน้อย\n",
    "3 = ถูกต้องบางส่วน มีข้อมูลหลักถูกต้อง\n",
    "2 = ถูกต้องเล็กน้อย ขาดข้อมูลสำคัญมาก\n",
    "1 = ไม่ถูกต้อง หรือไม่เกี่ยวข้อง\n",
    "\n",
    "คำถาม: {question}\n",
    "\n",
    "คำตอบที่ถูกต้อง: {ground_truth_answer}\n",
    "\n",
    "คำตอบที่ต้องประเมิน: {generated_answer[:500]}\n",
    "\n",
    "ตอบเป็นตัวเลข 1-5 เท่านั้น:\"\"\"\n",
    "    \n",
    "    response = ask_qwen3(prompt, temperature=0.1)\n",
    "    # Extract score\n",
    "    numbers = re.findall(r'[1-5]', response)\n",
    "    return int(numbers[0]) if numbers else 3  # Default to 3 if parsing fails\n",
    "\n",
    "\n",
    "print(\"Evaluation framework ready.\")\n",
    "print(f\"Q&A dataset: {len(qa_dataset)} questions\")\n",
    "print(f\"Question types: {Counter(q['question_type'] for q in qa_dataset)}\")\n",
    "print(f\"Difficulty levels: {Counter(q['difficulty'] for q in qa_dataset)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Run Evaluation & Compare\n",
    "\n",
    "Execute all 20 questions through both pipelines and collect metrics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run full evaluation\n",
    "results_a = []\n",
    "results_b = []\n",
    "\n",
    "print(\"Running evaluation on 20 questions...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, qa in enumerate(qa_dataset):\n",
    "    qid = qa[\"id\"]\n",
    "    question = qa[\"question\"]\n",
    "    gt_answer = qa[\"ground_truth_answer\"]\n",
    "    gt_sections = qa[\"reference_sections\"]\n",
    "    \n",
    "    print(f\"\\n[{i+1}/20] {qid}: {question[:60]}...\")\n",
    "    \n",
    "    # Pipeline A\n",
    "    print(\"  Pipeline A...\", end=\" \")\n",
    "    res_a = pipeline_a(question)\n",
    "    metrics_a = compute_retrieval_metrics(res_a[\"retrieved_sections\"], gt_sections)\n",
    "    score_a = llm_judge_answer(question, res_a[\"answer\"], gt_answer)\n",
    "    res_a.update({\n",
    "        \"qid\": qid, \"question\": question, \"gt_answer\": gt_answer,\n",
    "        \"gt_sections\": gt_sections, \"question_type\": qa[\"question_type\"],\n",
    "        \"difficulty\": qa[\"difficulty\"],\n",
    "        \"retrieval_precision\": metrics_a[\"precision\"],\n",
    "        \"retrieval_recall\": metrics_a[\"recall\"],\n",
    "        \"retrieval_f1\": metrics_a[\"f1\"],\n",
    "        \"answer_score\": score_a\n",
    "    })\n",
    "    results_a.append(res_a)\n",
    "    print(f\"Recall={metrics_a['recall']:.2f}, Score={score_a}, Time={res_a['latency']:.1f}s\")\n",
    "    \n",
    "    # Pipeline B\n",
    "    print(\"  Pipeline B...\", end=\" \")\n",
    "    res_b = pipeline_b(question)\n",
    "    metrics_b = compute_retrieval_metrics(res_b[\"retrieved_sections\"], gt_sections)\n",
    "    score_b = llm_judge_answer(question, res_b[\"answer\"], gt_answer)\n",
    "    res_b.update({\n",
    "        \"qid\": qid, \"question\": question, \"gt_answer\": gt_answer,\n",
    "        \"gt_sections\": gt_sections, \"question_type\": qa[\"question_type\"],\n",
    "        \"difficulty\": qa[\"difficulty\"],\n",
    "        \"retrieval_precision\": metrics_b[\"precision\"],\n",
    "        \"retrieval_recall\": metrics_b[\"recall\"],\n",
    "        \"retrieval_f1\": metrics_b[\"f1\"],\n",
    "        \"answer_score\": score_b\n",
    "    })\n",
    "    results_b.append(res_b)\n",
    "    print(f\"Recall={metrics_b['recall']:.2f}, Score={score_b}, Time={res_b['latency']:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluation complete!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build comparison DataFrame\n",
    "comparison_rows = []\n",
    "\n",
    "for ra, rb in zip(results_a, results_b):\n",
    "    comparison_rows.append({\n",
    "        \"qid\": ra[\"qid\"],\n",
    "        \"question\": ra[\"question\"][:50],\n",
    "        \"type\": ra[\"question_type\"],\n",
    "        \"difficulty\": ra[\"difficulty\"],\n",
    "        \"gt_sections\": \",\".join(ra[\"gt_sections\"]),\n",
    "        # Pipeline A\n",
    "        \"A_sections\": \",\".join(str(s) for s in ra[\"retrieved_sections\"]),\n",
    "        \"A_recall\": ra[\"retrieval_recall\"],\n",
    "        \"A_precision\": ra[\"retrieval_precision\"],\n",
    "        \"A_score\": ra[\"answer_score\"],\n",
    "        \"A_latency\": ra[\"latency\"],\n",
    "        # Pipeline B\n",
    "        \"B_sections\": \",\".join(str(s) for s in rb[\"retrieved_sections\"]),\n",
    "        \"B_recall\": rb[\"retrieval_recall\"],\n",
    "        \"B_precision\": rb[\"retrieval_precision\"],\n",
    "        \"B_score\": rb[\"answer_score\"],\n",
    "        \"B_latency\": rb[\"latency\"],\n",
    "        \"B_kg_added\": \",\".join(str(s) for s in rb.get(\"kg_added_sections\", []))\n",
    "    })\n",
    "\n",
    "df_compare = pd.DataFrame(comparison_rows)\n",
    "print(\"Comparison table:\")\n",
    "df_compare[[\"qid\", \"type\", \"difficulty\", \"A_recall\", \"A_score\", \"B_recall\", \"B_score\"]].to_string()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Overall summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = {\n",
    "    \"Pipeline A (PageIndex)\": {\n",
    "        \"Avg Retrieval Recall\": df_compare[\"A_recall\"].mean(),\n",
    "        \"Avg Retrieval Precision\": df_compare[\"A_precision\"].mean(),\n",
    "        \"Avg Answer Score (1-5)\": df_compare[\"A_score\"].mean(),\n",
    "        \"Avg Latency (s)\": df_compare[\"A_latency\"].mean(),\n",
    "    },\n",
    "    \"Pipeline B (PageIndex+KG)\": {\n",
    "        \"Avg Retrieval Recall\": df_compare[\"B_recall\"].mean(),\n",
    "        \"Avg Retrieval Precision\": df_compare[\"B_precision\"].mean(),\n",
    "        \"Avg Answer Score (1-5)\": df_compare[\"B_score\"].mean(),\n",
    "        \"Avg Latency (s)\": df_compare[\"B_latency\"].mean(),\n",
    "    }\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary).T\n",
    "print(df_summary.to_string())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Breakdown by question type\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BREAKDOWN BY QUESTION TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for qtype in df_compare[\"type\"].unique():\n",
    "    subset = df_compare[df_compare[\"type\"] == qtype]\n",
    "    print(f\"\\n{qtype} ({len(subset)} questions):\")\n",
    "    print(f\"  Pipeline A - Recall: {subset['A_recall'].mean():.2f}, Score: {subset['A_score'].mean():.1f}\")\n",
    "    print(f\"  Pipeline B - Recall: {subset['B_recall'].mean():.2f}, Score: {subset['B_score'].mean():.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Breakdown by difficulty\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BREAKDOWN BY DIFFICULTY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    subset = df_compare[df_compare[\"difficulty\"] == diff]\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "    print(f\"\\n{diff} ({len(subset)} questions):\")\n",
    "    print(f\"  Pipeline A - Recall: {subset['A_recall'].mean():.2f}, Score: {subset['A_score'].mean():.1f}\")\n",
    "    print(f\"  Pipeline B - Recall: {subset['B_recall'].mean():.2f}, Score: {subset['B_score'].mean():.1f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Pipeline comparison bar charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Overall metrics comparison\n",
    "metrics_names = [\"Retrieval Recall\", \"Retrieval Precision\", \"Answer Score\\n(normalized)\"]\n",
    "a_vals = [df_compare[\"A_recall\"].mean(), df_compare[\"A_precision\"].mean(), df_compare[\"A_score\"].mean() / 5]\n",
    "b_vals = [df_compare[\"B_recall\"].mean(), df_compare[\"B_precision\"].mean(), df_compare[\"B_score\"].mean() / 5]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, a_vals, width, label=\"Pipeline A\", color=\"#4A90D9\")\n",
    "axes[0].bar(x + width/2, b_vals, width, label=\"Pipeline B\", color=\"#E74C3C\")\n",
    "axes[0].set_xlabel(\"Metric\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_title(\"Overall Comparison\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics_names, fontsize=9)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# 2. Answer score by question type\n",
    "types = df_compare[\"type\"].unique()\n",
    "a_scores_by_type = [df_compare[df_compare[\"type\"]==t][\"A_score\"].mean() for t in types]\n",
    "b_scores_by_type = [df_compare[df_compare[\"type\"]==t][\"B_score\"].mean() for t in types]\n",
    "\n",
    "x = np.arange(len(types))\n",
    "axes[1].bar(x - width/2, a_scores_by_type, width, label=\"Pipeline A\", color=\"#4A90D9\")\n",
    "axes[1].bar(x + width/2, b_scores_by_type, width, label=\"Pipeline B\", color=\"#E74C3C\")\n",
    "axes[1].set_xlabel(\"Question Type\")\n",
    "axes[1].set_ylabel(\"Avg Score (1-5)\")\n",
    "axes[1].set_title(\"Score by Question Type\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(types, rotation=30, ha=\"right\", fontsize=9)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 5.5)\n",
    "\n",
    "# 3. Answer score by difficulty\n",
    "diffs = [\"easy\", \"medium\", \"hard\"]\n",
    "a_scores_by_diff = [df_compare[df_compare[\"difficulty\"]==d][\"A_score\"].mean() for d in diffs]\n",
    "b_scores_by_diff = [df_compare[df_compare[\"difficulty\"]==d][\"B_score\"].mean() for d in diffs]\n",
    "\n",
    "x = np.arange(len(diffs))\n",
    "axes[2].bar(x - width/2, a_scores_by_diff, width, label=\"Pipeline A\", color=\"#4A90D9\")\n",
    "axes[2].bar(x + width/2, b_scores_by_diff, width, label=\"Pipeline B\", color=\"#E74C3C\")\n",
    "axes[2].set_xlabel(\"Difficulty\")\n",
    "axes[2].set_ylabel(\"Avg Score (1-5)\")\n",
    "axes[2].set_title(\"Score by Difficulty\")\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(diffs, fontsize=9)\n",
    "axes[2].legend()\n",
    "axes[2].set_ylim(0, 5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/pipeline_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: data/results/pipeline_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Per-question score comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "x = np.arange(len(qa_dataset))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, df_compare[\"A_score\"], width, label=\"Pipeline A (PageIndex)\", color=\"#4A90D9\", alpha=0.8)\n",
    "ax.bar(x + width/2, df_compare[\"B_score\"], width, label=\"Pipeline B (PageIndex+KG)\", color=\"#E74C3C\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Question ID\")\n",
    "ax.set_ylabel(\"Answer Quality Score (1-5)\")\n",
    "ax.set_title(\"Per-Question Answer Quality Comparison\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_compare[\"qid\"], rotation=45, ha=\"right\", fontsize=8)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/results/per_question_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: data/results/per_question_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Analysis & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Qualitative analysis: Show 3 questions where pipelines differ most\n",
    "df_compare[\"score_diff\"] = df_compare[\"B_score\"] - df_compare[\"A_score\"]\n",
    "\n",
    "# Cases where KG helped\n",
    "kg_helped = df_compare.nlargest(2, \"score_diff\")\n",
    "# Cases where KG hurt\n",
    "kg_hurt = df_compare.nsmallest(1, \"score_diff\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUALITATIVE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Cases where KG HELPED ---\")\n",
    "for _, row in kg_helped.iterrows():\n",
    "    qid = row[\"qid\"]\n",
    "    ra = next(r for r in results_a if r[\"qid\"] == qid)\n",
    "    rb = next(r for r in results_b if r[\"qid\"] == qid)\n",
    "    print(f\"\\n{qid} [{row['type']}, {row['difficulty']}]: {ra['question']}\")\n",
    "    print(f\"  Ground truth sections: {ra['gt_sections']}\")\n",
    "    print(f\"  Pipeline A sections: {ra['retrieved_sections']} → Score: {row['A_score']}\")\n",
    "    print(f\"  Pipeline B sections: {rb['retrieved_sections']} → Score: {row['B_score']}\")\n",
    "    print(f\"  KG added: {rb.get('kg_added_sections', [])}\")\n",
    "    print(f\"  Pipeline A answer: {ra['answer'][:200]}...\")\n",
    "    print(f\"  Pipeline B answer: {rb['answer'][:200]}...\")\n",
    "\n",
    "print(\"\\n--- Cases where KG HURT (or no help) ---\")\n",
    "for _, row in kg_hurt.iterrows():\n",
    "    qid = row[\"qid\"]\n",
    "    ra = next(r for r in results_a if r[\"qid\"] == qid)\n",
    "    rb = next(r for r in results_b if r[\"qid\"] == qid)\n",
    "    print(f\"\\n{qid} [{row['type']}, {row['difficulty']}]: {ra['question']}\")\n",
    "    print(f\"  Ground truth sections: {ra['gt_sections']}\")\n",
    "    print(f\"  Pipeline A sections: {ra['retrieved_sections']} → Score: {row['A_score']}\")\n",
    "    print(f\"  Pipeline B sections: {rb['retrieved_sections']} → Score: {row['B_score']}\")\n",
    "    print(f\"  KG added: {rb.get('kg_added_sections', [])}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summary discussion\n",
    "avg_a_score = df_compare[\"A_score\"].mean()\n",
    "avg_b_score = df_compare[\"B_score\"].mean()\n",
    "avg_a_recall = df_compare[\"A_recall\"].mean()\n",
    "avg_b_recall = df_compare[\"B_recall\"].mean()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISCUSSION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "winner = \"Pipeline B (PageIndex + KG)\" if avg_b_score > avg_a_score else \"Pipeline A (PageIndex)\"\n",
    "if avg_a_score == avg_b_score:\n",
    "    winner = \"Tie\"\n",
    "\n",
    "print(f\"\\n1. Overall Winner: {winner}\")\n",
    "print(f\"   A avg score: {avg_a_score:.2f} vs B avg score: {avg_b_score:.2f}\")\n",
    "print(f\"   A avg recall: {avg_a_recall:.2f} vs B avg recall: {avg_b_recall:.2f}\")\n",
    "\n",
    "# Find where KG helps most\n",
    "for qtype in df_compare[\"type\"].unique():\n",
    "    subset = df_compare[df_compare[\"type\"] == qtype]\n",
    "    diff = subset[\"B_score\"].mean() - subset[\"A_score\"].mean()\n",
    "    if abs(diff) >= 0.5:\n",
    "        direction = \"helps\" if diff > 0 else \"hurts\"\n",
    "        print(f\"\\n2. KG {direction} for '{qtype}' questions (diff: {diff:+.1f})\")\n",
    "\n",
    "print(f\"\\n3. Latency:\")\n",
    "print(f\"   Pipeline A avg: {df_compare['A_latency'].mean():.1f}s\")\n",
    "print(f\"   Pipeline B avg: {df_compare['B_latency'].mean():.1f}s\")\n",
    "print(f\"   KG overhead: {df_compare['B_latency'].mean() - df_compare['A_latency'].mean():.1f}s\")\n",
    "\n",
    "print(f\"\\n4. Limitations:\")\n",
    "print(f\"   - Small test set (20 questions)\")\n",
    "print(f\"   - LLM-as-judge may have biases\")\n",
    "print(f\"   - 1-hop KG expansion may miss deeper connections\")\n",
    "print(f\"   - Qwen3:8b may have limited Thai legal understanding\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save all results\n",
    "\n",
    "# Pipeline A results\n",
    "results_a_serializable = []\n",
    "for r in results_a:\n",
    "    r_copy = {k: v for k, v in r.items() if k != \"retrieved_names\"}\n",
    "    results_a_serializable.append(r_copy)\n",
    "\n",
    "with open(RESULTS_DIR / \"pipeline_a_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_a_serializable, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Pipeline B results\n",
    "results_b_serializable = []\n",
    "for r in results_b:\n",
    "    r_copy = {k: v for k, v in r.items() if k != \"retrieved_names\"}\n",
    "    results_b_serializable.append(r_copy)\n",
    "\n",
    "with open(RESULTS_DIR / \"pipeline_b_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_b_serializable, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Comparison CSV\n",
    "df_compare.to_csv(RESULTS_DIR / \"evaluation_comparison.csv\", index=False)\n",
    "\n",
    "# Summary JSON\n",
    "evaluation_summary = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"num_questions\": len(qa_dataset),\n",
    "    \"pipeline_a\": {\n",
    "        \"name\": \"PageIndex Only\",\n",
    "        \"avg_retrieval_recall\": float(df_compare[\"A_recall\"].mean()),\n",
    "        \"avg_retrieval_precision\": float(df_compare[\"A_precision\"].mean()),\n",
    "        \"avg_answer_score\": float(df_compare[\"A_score\"].mean()),\n",
    "        \"avg_latency_seconds\": float(df_compare[\"A_latency\"].mean()),\n",
    "    },\n",
    "    \"pipeline_b\": {\n",
    "        \"name\": \"PageIndex + Light KG\",\n",
    "        \"avg_retrieval_recall\": float(df_compare[\"B_recall\"].mean()),\n",
    "        \"avg_retrieval_precision\": float(df_compare[\"B_precision\"].mean()),\n",
    "        \"avg_answer_score\": float(df_compare[\"B_score\"].mean()),\n",
    "        \"avg_latency_seconds\": float(df_compare[\"B_latency\"].mean()),\n",
    "    },\n",
    "    \"by_question_type\": {},\n",
    "    \"by_difficulty\": {}\n",
    "}\n",
    "\n",
    "for qtype in df_compare[\"type\"].unique():\n",
    "    subset = df_compare[df_compare[\"type\"] == qtype]\n",
    "    evaluation_summary[\"by_question_type\"][qtype] = {\n",
    "        \"count\": len(subset),\n",
    "        \"A_avg_score\": float(subset[\"A_score\"].mean()),\n",
    "        \"B_avg_score\": float(subset[\"B_score\"].mean()),\n",
    "        \"A_avg_recall\": float(subset[\"A_recall\"].mean()),\n",
    "        \"B_avg_recall\": float(subset[\"B_recall\"].mean()),\n",
    "    }\n",
    "\n",
    "for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "    subset = df_compare[df_compare[\"difficulty\"] == diff]\n",
    "    if len(subset) > 0:\n",
    "        evaluation_summary[\"by_difficulty\"][diff] = {\n",
    "            \"count\": len(subset),\n",
    "            \"A_avg_score\": float(subset[\"A_score\"].mean()),\n",
    "            \"B_avg_score\": float(subset[\"B_score\"].mean()),\n",
    "            \"A_avg_recall\": float(subset[\"A_recall\"].mean()),\n",
    "            \"B_avg_recall\": float(subset[\"B_recall\"].mean()),\n",
    "        }\n",
    "\n",
    "with open(RESULTS_DIR / \"evaluation_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"All results saved:\")\n",
    "for f in RESULTS_DIR.iterdir():\n",
    "    size = f.stat().st_size\n",
    "    print(f\"  {f.name} ({size:,} bytes)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Task 3: Model Development — COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Questions evaluated: {len(qa_dataset)}\")\n",
    "print(f\"\\nPipeline A (PageIndex):       Score = {df_compare['A_score'].mean():.2f}/5\")\n",
    "print(f\"Pipeline B (PageIndex + KG):  Score = {df_compare['B_score'].mean():.2f}/5\")\n",
    "print(f\"\\nOutput files in: {RESULTS_DIR}/\")\n",
    "print(f\"Node summaries cached in: {SUMMARIES_PATH}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}